\documentclass[english, 12pt]{article}
\usepackage{yingconfig}

% ========================Variables======================================
\newcommand{\coursecode}{MATH 136}
\newcommand{\coursename}{Linear Algebra}
\newcommand{\thisprof}{Professor M. Rubinstein}
\newcommand{\curterm}{Winter 2014}

\begin{document}
\notesheader

\section{Vectors in Euclidean Space}

\subsection{Vector Addition and Scalar Multiplication}


\begin{defn}
$\R^n$ consists of n-tuples of real numbers, where $n \in \N$.
\end{defn}

\begin{defn}
\textbf{Points/vectors} are elements of $\R^n$.
\end{defn}
\subsubsection*{Notation}
\[\R^n = \{(x_{1},x_{2},\dots,x_{n})\,|\, x_{1},x_{2},\dots,x_{n} \in \R \}\]
\[x_{1} + x_{2} = 3\]
\[2 x_{1} + 5 x_{2} = 4\]
\[
\begin{bmatrix} 1 & 1 \\ 2 & 5 \end{bmatrix}
\begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} =
\begin{bmatrix} 3\\ 4 \end{bmatrix} \]
Two vectors in $\R^n$ are equal if all coordinates are equal.
\subsubsection*{Vector Operations}
Let $\vx \in \R^n, \alpha \in \R$
\begin{align*}
& \text{Addition} & \text{Scalar Multiplication} \\
& \vx + \vy =
\begin{bmatrix}
x_{1} + y_{1} \\
x_{2} + y_{2} \\
\vdots \\
x_{n} + y_{n}
\end{bmatrix} \in \R^n &
\alpha \vx =
\begin{bmatrix}
\alpha x_{1} \\
\alpha x_{2} \\
\vdots \\
\alpha x_{n}
\end{bmatrix} \in \R^n
\end{align*}

\begin{defn}
$\vec{0}$ is the \textbf{additive identity}.
\end{defn}

\begin{defn}
Given a vector $\vx \in \R^n$, $-\vx$ is the \textbf{additive inverse}.
\end{defn}

\begin{defn}
A sum of scalar multiples of a combination of vectors is a \textbf{linear combination}
\[c_{1} \vv_{1} + c_{2} \vv_{2} + \cdots + c_{k} \vv_{k}:\,c_{1}\dots c_{k} \in \R\]
\end{defn}

\begin{thrm}
If $\vx,\vy,\vw \in \R^n$, and $c,d \in \R$, then
\begin{itemize}
\item $\vx + \vy \in \R^n$
\item $(\vx + \vy) + \vw = \vx + (\vy + vw)$
\item $\vx + \vy = \vy + \vx$
\item $\exists\, \vec{0} \in \R^n$ such that $\vx + \vec{0} = \vx\qquad\forall \vx \in \R^n$
\item $\forall \vx \in \R^n$, there exists a vector $(- \vx) \in \R^n$ such that $\vx + (- \vx) = \vec{0}$
\item $c \vx \in \R^n$
\item $c (d \vx) = (cd) \vx$
\item $(c+d) \vx = c \vx + d \vy$
\item $c (\vx + vy) = c \vx + c \vy$
\item $1 \vx = \vx$
\end{itemize}
\end{thrm}

\begin{defn}
The set $S$ of all possible linear combinations of a set of vectors $B = (\vec{v}_{1} , \dots , \vec{v}_{k})$ in $\R^n$ is called the \textbf{span} of the set $B$ and we write
\[S = \text{Span B} =\{t_{1} \vec{v}_{1} + t_{2} \vv_{2} + \cdots + t_{k}\vv_{k}\}\]
$S$ is \textbf{spanned} by B and that B is a spanning set for $S$.
\end{defn}

\begin{note}
A set in the form
\[  \{t_{1} \vec{v}_{1} + \dots + t_{k} \vec{v}_{k} + \vec{b} | t_{1},\dots,t_{k} \in \R \}\]
can be written as
\[  \vx = t_{1} \vec{v}_{1} + \dots + t_{k} \vec{v}_{k} + \vec{b} , t_{1},\dots,t_{k} \in \R\]
In $\R^n$, two linearly independent vectors $\vx_{1}$ and $\vx_{2}$ generate a plane.
\end{note}

\begin{thrm}
If $\vv_{k}$ can be written as a linear combination of $\vv_{1},\dots,\vv_{k-1}$, then
\[\spn\{\vv_{1},\dots,\vv_{k}\} = \spn\{\vv_{1},\dots,\vv_{k-1}\}\]
\end{thrm}

\begin{defn}
A set of vectors in $\R^n$ is said to be \textbf{linearly dependent} if there exists coefficients $c_{1},\dots,c_{k}$, not all $0$, such that
\[\vec{0} = c_{1} \vec{v}_{1} + \cdots + c_{k} \vec{v}_{k}\]
Either $0$ vector or two or more vectors are colinear (scalar multiple).
\end{defn}

\begin{defn}
A set of vectors is \textbf{linearly independent} if the only solution is $c_{1} = c_{2} = \cdots = c_{k} = 0$ (\textbf{trivial solution})
\end{defn}

\begin{thrm}
If a set of vectors $\{\vv_{1},\dots,\vv_{k}\}$ contains the zero evctor, then it is linearly dependent.
\end{thrm}

\begin{defn}
If a subset of $\R^n$ can be written as a span of vectors $\vec{v}_{1},\dots,\vec{v}_{k}$ where $\{\vec{v}_{1},\dots,\vec{v}_{k}\}$ is linearly independent, then $\{\vec{v}_{1},\dots,\vec{v}_{k}\}$ is a \textbf{basis} for $S$. The basis of the set $\{\vec{0}\}$ is the empty set.
\end{defn}

\begin{thrm}
If $\beta= \{\vv_{1},\dots,\vv_{k}\}$ is a basis for a subset $S$ of $\R^n$, then every vector $\vx \in S$ can be written as unique linear combination of $\vv_{1},\dots,\vv_{k}$.
\end{thrm}

\begin{defn}
The \textbf{standard basis} in $\R^n$ is a set of vectors where each vector's ith component is $1$, and all other components are $0$.
\end{defn}

\begin{defn}
Let $\vx,vy \in \R^n$. The set with vector equation $\vw = c_{1} \vx + \vy$ with $ c_{1} \in \R$ is a \textbf{line} in $\R^n$ that passes through $\vy$.
\end{defn}

\begin{defn}
Let $\vv_{1},\vv_{2},\vy \in \R^n$ with $\{\vv_{1},\vv_{2}\}$ being a linearly independent set. The set with the vector equation $\vx = c_{1} \vv_{1} + c_{2} \vv_{2} + \vy$ with $c_{1},c_{2} \in \R$ is a \textbf{plane} in $\R^n$ which passes through $\vy$.
\end{defn}

\begin{defn}
Let $\vv_{1},\dots,\vv_{k},\vy \in \R^n$ with the set being linearly independent. The set with the vector equation $\vx = c_{1} v_{1} + \cdots + c_{k} \vv_{k} + \vy$ with $c_{1},\dots,c_{k}$ is a \textbf{k-plane} in $R^n$ with passes through $\vy$.
\end{defn}

\begin{defn}
A \textbf{hyperplane} is a subspace of one dimension less than its ambient space.
\end{defn}

\subsection{Subspaces}

\begin{thrm}
\textbf{Subspace Test}: Let $\mathbb{S}$ be a non-empty subset of $\R^n$. If $\vx + \vy \in \mathbb{S}$ and $c \vx \in \mathbb{S}$ for all $\vx,\vy \in \mathbb{S}$ and $c \in \R$, then $\mathbb{S}$ is a subspace of $\R^n$
\end{thrm}

\begin{qte}
If $\vec{0}$ is not in the set, definitely not subset. If it is, further investigation needed.
\end{qte}

\begin{defn}
$S \in \R^n$ is closed under scalar multiplication if for all $\vx \in S$ and $\alpha \in \R$, $\alpha \vx \in S$.
\end{defn}

\begin{thrm}
If $\{\vv_{1},\dots,\vv_{k}\}$ is a set of vectors in $\R^n$, then $\spn \{\vv_{1},\dots,\vv_{k}\}$ is a subspace of $\R^n$.
\end{thrm}

\subsection{Dot Product}

\begin{thrm}
If $\vx,\vy \in \R^2$, and $\theta$ is the angle between them, then
\[\vx \cdot \vy = ||\vx||\,||\vy||\, \cos \theta\]
\end{thrm}

\begin{defn}
Given two vectors $\vx,vy$, their dot product is defined by
\[\vx \dot \vy = x_{1} y_{1} + x_{2} + y_{2} + \cdots + x_{n} y_{n} = \sum_{i=1}^n x_{i} y_{i}\]
\end{defn}

\begin{thrm}
Let $\vx,\vy\,\vz \in \R^n$ and let $s,t \in \R$. Then
\begin{itemize}
\item $\vx \cdot \vx \geq 0$ and $\vx \cdot \vx = 0$ if and only if $\vx = \vec{0}$
\item $\vx \cdot \vy = \vy \cdot \vx$
\item $\vx \cdot (s\vy + t \vz) = s (\vx \cdot \vy) + t (\vx \cdot \vz)$
\end{itemize}
\end{thrm}

\begin{thrm}
If $\vx \cdot \vy = 0$, then $\vx$ and $\vy$ are \textbf{orthogonal}.
\end{thrm}
\begin{qte}
The zero vector $\vec{0} \in \R^n$ is orthogonal to every vector in $\R^n$.
\end{qte}

\begin{thrm}
The \textbf{cross product} of $\vx,\vy \in \R^3$ is given by
\[\vx \times \vy =
\begin{bmatrix}
x_{2}y_{3} - x_{3} y_{2} \\
- (x_{1} y_{3} - x_{3} y_{1}) \\
x_{1} y_{2} - x_{2} y_{1}
\end{bmatrix}\]
\end{thrm}

\begin{qte}
Cross product is not associative. $\vv \times (\vw \times \vx) \neq (\vv \times \vw) \times \vx$.
\end{qte}

\begin{thrm}
Let $\vv,\vw, \vec{b} \in \R^3$ with $\{\vv,\vw\}$ being a linear independent set, and define $\vec{n} = \vv \times \vw$. If $P$ is a plane with the vector equation
\[ \vx = c \vv + d \vw + \vec{b}, \qquad c,d \in \R\]
then an alternate equation for the plane is
\[(\vx - \vec{b}) \cdot \vec{n} = 0\]
$n$ is a normal vector to the plane $P$.
Rearranging: $n_{1} x_{1} + n_{2} x_{2} + n_{3} x_{3} = n_{1} a_{1} + n_{2} a_{2} + n_{3} a_{3}$.
\end{thrm}

\subsection{Projections}
\begin{defn}
Let $\vu,\vv \in \R^n$ with $\vv \neq \vec{0}$. The \textbf{projection of $\vu$ onto $\vv$} is
\[\operatorname{proj}_{\vv} (\vu) = \f{\vu \cdot \vv}{||\vv||^2} \vv\]
\end{defn}

\begin{defn}
The \textbf{perpendicular of $\vu$ onto $\vv$} is
\[\operatorname{perp}_{\vv}(\vu) = \vu - \operatorname{proj}_{\vv}(\vu)\]
\end{defn}

\begin{qte}
To project a vector onto a plane, take the perpendicular of the vector projected onto the normal of the plane.
\end{qte}

\section{Systems of Linear Equations}

\subsection{Systems of Linear Equations}

\begin{defn}
A {system of linear equations in $n$ variables}
\begin{equation}
c x_{1} + c x_{2} + \cdots + c x_{n} = b_{1}
\end{equation}
\begin{equation}
c x_{1} + c x_{2} + \cdots + c x_{n} = b_{2}
\end{equation}
\begin{equation}
c x_{1} + c x_{2} +\cdots + c x_{n} = b_{3}
\end{equation}
$\vec{s} = \begin{bmatrix} s_{1} \\ s_{2} \\ \vdots \end{bmatrix} \in \R^n$ is a solution to the system if all the equations are satisfied when $x_{i}$ is set to $s_{i}$.\\\\
If a system has a solution, it is \textbf{consistent}. If not, it is \textbf{inconsistent}.
\end{defn}

\begin{thrm}
Assume the system of linear equations with $a_{1},\dots,a_{n}, b \in \R$ has two distinct solutions $\vec{s} = \begin{bmatrix} s_{1} \\ \vdots \\ s_{n} \end{bmatrix}$ and $\vec{t} = \begin{bmatrix} t_{1} \\ \vdots \\ t_{n} \end{bmatrix}$. Then $\vx = \vec{s} + c(\vec{s} - \vec{t})$ is a distinct solution for each $c \in \R$.
\end{thrm}

\begin{defn}
A \textbf{solution set} is the set of all solutions of a system of linear equations. Two systems of equations are equivalent if they have the same solution set.
\end{defn}

\subsection{Solving Systems of Linear Equation}

\begin{defn}
The \textbf{coefficient matrix} of a system is denoted by $A = \begin{bmatrix} a_{11}  & a_{21} & \cdots \\ a_{21} & a_{22} & \cdots \end{bmatrix}$.
\end{defn}

\begin{defn}
The \textbf{augment matrix} is
\[
\begin{bmatrix}[cc|c]
  a_{11} & a_{12} & b_{1}\\
  a_{21} & a_{22} & b_{2}
\end{bmatrix}
\]
\end{defn}

\begin{mthd}
The three \textbf{elementary row operations} for solving a system of linear equations are:
\begin{enumerate}
\item Multiplying a row by a scalar
\item Adding a multiple of one row to another
\item Swapping two rows
\end{enumerate}
\end{mthd}

\begin{thrm}
If two augmented matrices are row equivalent, then the system of linear equations associated with each matrix are equivalent.
\end{thrm}

\begin{defn}
A matrix is said to be in \textbf{reduced row echelon form} (RREF) if:
\begin{enumerate}
  \item All rows containing a non-zero entry are above rows which only contain zeroes.
  \item The first non-zero entry in each row is 1. (\textbf{leading one}).
  \item Leading one on each zero row is to the right of the leading one on any row above it.
  \item Leading one is the only non-zero entry in its column.
\end{enumerate}
\end{defn}

\begin{thrm}
The RREF of a matrix is unique.
\end{thrm}

\begin{defn}
Let $R$ be the RREF of a coefficient matrix of a system of linear equations. If the jth column does not contain a leading one, $x_{j}$ is a \textbf{free variable}.
\end{defn}

\begin{defn}
The \textbf{rank} of a matrix is the number of leading ones in the RREF of the matrix.
\end{defn}

\begin{thrm}
Let $A$ be the $m \times n$ coefficient matrix of a system of linear equations.
\begin{enumerate}
\item IF the rank of $A$ is less than the rank for the augmented matrix, then the system is inconsistent.
\item If the system is inconsistent, then the system contains $n - \text{ rank}\,A$ free variables. A consistent system has a unique solution if and only if rank $A=n$.
\item rank $A=m$ if and only if the system is consistent for every $\vec{b} \in \R^m$
\end{enumerate}
\end{thrm}

\begin{defn}
A system of linear equations is said to be \textbf{homogeneous system} if the right-hand side only contains zeroes. It has the form $\begin{bmatrix} A\, |\, \vec{0} \end{bmatrix}$.
\end{defn}

\begin{thrm}
The solution set of a homogeneneous systems of $M$ linear equations in $n$ variables is a subspace of $R^n$.
\end{thrm}

\section{Matrices and Linear Mappings}

\subsection{Operations on Matrices}

\begin{defn}
A $m \times n$ \textbf{matrix} is a rectangular array with $m$ rows and $n$ columns.
\end{defn}

\begin{defn}
\textbf{Addition and scalar multiplication of matrices:} Let $A,B \in M_{m \times n} (\R)$ and $c \in \R$. $A+B$ and $cA$ are defined as
\[(A + B)_{ij} = (A)_{ij} + (B)_{ij}\]
\[(cA)_{ij} = c(A)_{ij}\]
\end{defn}

\begin{thrm}
Let $A,B,C$ be $m \times n$ matrices and let $c,d \in \R$
\begin{enumerate}
\item $A + B$ is an $m \times n$ matrix
\item $(A + B) + C = A + (B + C)$
\item $A + B = B + A$
\item There exists a matrix such that $A + O_{m,n} = A$. This is called the \textbf{zero matrix}
\item There exists a matrix $(-A)$ such that $A + (-A) = O_{m,n}$
\item $cA \in M_{m \times n}$
\item $c(dA) = cd(A)$
\item $(c + d)A = cA + dA$
\item $c(A + B) = cA + cB$
\item $1A = A$
\end{enumerate}
\end{thrm}

\begin{defn}
The \textbf{zero matrix}, denoted as $O_{m,n}$ is the matrix whose entries are all $0$.
\end{defn}

\begin{defn}
The \textbf{transpose} of an $m \times n$ matrix $A$ is the $n \times m$ matrix $A^T$ whose ij-th entry is the ji-th entry of $A$.
\[ (A^T)_{ij} = (A)_{ji}\]
\end{defn}

\begin{thrm}
For any $m \times n$ matrices $A$ and $B$ and scalar $c \in \R$,
\begin{itemize}
\item $(A^T)^T = A$
\item $(A+B)^T = A^T + B^T$
\item $(cA)^T = c(A^T)$
\end{itemize}
\end{thrm}

\begin{defn}
\textbf{Matrix-Vector multiplication:} Let $A$ be an $m \times n$ matrix whose rows are denoted $\vec{a}_{i}^T$ for $1 \leq i \leq m$. Then, for any $\vx \in \R^n$, we define
\[ A \vx = \begin{bmatrix} \vec{a}_{1} \cdot \vx \\ \vdots \\ \vec{a}_{m} \cdot \vx \end{bmatrix} \]
An alternate form is
\[ A \vx = x_{1} \begin{bmatrix} a_{11} \\ \vdots \\ a_{m1} \end{bmatrix} + \cdots + x_{n} \begin{bmatrix} a_{1n} \\ \vdots \\ a_{mn} \end{bmatrix} \]
\end{defn}

\begin{thrm}
If $\vec{e}_{i}$ is the ith standard basis vector for $\R^i$ and $A = \lbrack \vec{a}_{1},\dots,\vec{a}_{n} \rbrack$ is an $m \times n$ matrix, then
\[A \vec{e}_i = \vec{a}_{i}\]
\end{thrm}

\begin{mthd}
\textbf{Matrix Multiplication:} Let $A$ be an $m \times n$ matrix and let $B = \lbrack \vec{b}_{1}\, \cdots\, \vec{b}_{p} \rbrack$ be an $n \times p$ matrix. Then, $AB$ is the $m \times p$ matrix
\[ AB = \lbrack A \vec{b}_{1} \, \cdots \, A \vec{b}_{p}\rbrack \]
\end{mthd}

\begin{note}
The number of columns of $A$ must equal to the number of rows of $B$ for this to be defined. The resulting matrix will have the same rows as $A$ and same columns as $B$.
\end{note}

\begin{thrm}
If $A,B,C$ are matrices of the correct size so the required products are defined and $t \in \R$, then
\begin{itemize}
\item $A(B+C) = AB _ AC$
\item $t (AB) = (tA)B + A(tB)$
\item $A(BC) = (AB)C$
\item $(AB)^T = B^T A^T$
\end{itemize}
\end{thrm}

\begin{qte}
Matrix Multiplication is NOT commutative. $AB \neq BA$., if $AB = AC$, $B \neq C$.
\end{qte}

\begin{thrm}
Suppose that $A$ and $B$ are $m \times n$ matrices such that $A\vx = B\vx$ for every $\vx \in \R^n$, then $A=B$.
\end{thrm}

\begin{defn}
The $n \times n$ \textbf{identity matrix}, denoted as $I$, is the matrix containing a diagonal row of $1$s and everything else set to $0$. The columns of $I_{n}$ are the standard basis vectors of $\R^n$. \\\\
For every $n \times n$ matrix, $A$, $AI = A = IA$.
\end{defn}

\begin{thrm}
If $I$ is the matrix $I = \lbrack \vec{e}_1,\dots,\vec{e}_n \rbrack$ then for any $n \times n$ matrix where $IA = A = AI$
\end{thrm}

\begin{thrm}
The multiplicative identity for $M_{n\times n} (\R)$ is unique.
\end{thrm}

\begin{exmp}
\textbf{Block matrix}: Let $A = \begin{bmatrix} 1 & -1 & 3 & 4 \\ 0 & 0 & 0 & 0 \\ 0 & 3 & 1 & 2 \end{bmatrix} $. By reducing $A$ into blocks, we can write
\[ A = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix} \]
with $A_{11} = \begin{bmatrix} 1 & -1 \\ 0 & 0 \end{bmatrix}$, $A_{12} = \begin{bmatrix} 3 & 4 \\ 0 & 0 \end{bmatrix}$,
$A_{21} = \begin{bmatrix} 0 & 3 \end{bmatrix} $, $A_{22} = \begin{bmatrix} 1 & 2 \end{bmatrix}$. \\
These are useful to distribute matrix multiplication over multiple computers to speed up the process.
\end{exmp}

\subsection{Linear Mapping}

\begin{thrm}
Let $A$ be an $m \times n$ matrix, and let $F: \R^n \rightarrow \R^m$ be defined by $f(\vx) = A\vx$. Then for all $\vx,\vy \in \R^n$ and $b,c \in \R$ we have
\[f(b\vx + c\vy) = bf(\vx) + cf(\vy)\]
\end{thrm}

\begin{defn}
A function $L : \R^n \rightarrow \R^m$ is said to be a \textbf{linear mapping} if for every $\vx,\vy \in \R^n$ and $b,c \in \R$, we have
\[ L(b\vx + c \vy) = bL(\vx) + cL(\vy)\]
\end{defn}

\begin{note}
This definition can be used to prove linear mapping.
\end{note}

\begin{exmp}
Prove that the function $L: \R^3 \rightarrow \R^2$ defined by $L(x_{1},x_{2},x_{3}) = (3x_{1} - x_{2},2x_{1} + 2 x_{3})$ is a linear mapping.
\begin{sol}
\begin{align*}
L(b\vx + c \vy) &= L(b(x_{1},x_{2},x_{3}) + c(y_{1},y_{2},y_{3})) \\
&= L( b x_{1} + c y_{1}, b x_{2} + c y _{2}, b x_{3} + c y_{3})
&= (3 (b x_{1} + c y_{1}) - (b x_{2} + c y_{2})), 2 (b x_{1} + c y_{1}) + 2 (b x_{3} + c y_{3})
&= b(3 x_{1} - x_{2}, 2 x_{1} + 2 x_{3}) + c (3 y_{1} - y_{2},2 y_{1} + 2 y_{3})
&= bL(\vx) + c L(\vy)
\end{align*}
\end{sol}
\end{exmp}

\begin{thrm}
Every linear mapping can be represented as a matrix mapping whose columns are the images of the standard basis vector of $\R^n$ under $L$. $L(\vx) = \lbrack L \rbrack \vx$ where
\[\lbrack L \rbrack = \lbrack L(\vec{e}_{1}) \cdots L(\vec{e}_{n}) \rbrack\]
\end{thrm}

\todo{This theorem still confusing}

\begin{exmp}
Determine the standard matrix of $L(x_{1},x_{2},x_{3}) = (3x_{1} - x_{2} , 2 x_{1} + 2 x_{3})$
\begin{sol}
\[L(1,0,0) = (3,2)\]
\[ L(0,1,0) = (-1,0)\]
\[L (0,0,1) = (0,2)\]
\[ \lbrack L \rbrack = \lbrack L(\vec{e}_{1}) \qquad L(\vec{e}_{2}) \qquad L(\vec{e}_{3}) \rbrack = \begin{bmatrix} 3 & -1 & 0 \\ 2 & 0 & 2 \end{bmatrix}\]
\end{sol}
\end{exmp}

\begin{defn}
The \textbf{rotation} in $\R^2$ is
\[R_{\theta}(x_{1},x_{2}) = (x_{1} \cos \theta - x_{2} \sin \theta, x_{1} \sin \theta + x_{2} \cos \theta)\]
The standard matrix of $\R_{\theta}$ is
\[ \lbrack r_{\theta} \rbrack = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}\]
\end{defn}

\begin{thrm}
Let $R_{\theta}: \R^2 \rightarrow \R^2$ be a rotation with rotation matrix $A = \lbrack \R_{\theta} \rbrack$. Then the columns of $A$ are orthogonal unit vectors.
\end{thrm}

\begin{defn}
Let $\text{refl}_{\vec{n}}: \R^n \rightarrow \R^n$ denote the linear mapping which maps a vector $\vx$ to its mirror image in the hyperplane with normal vector $\vec{n}$. The reflection of $\vx$ over the line witth the normal vector $\vec{n}$ is given by
\[\text{refl}_{\vec{n}} = \vx - 2 \text{proj}_{\vec{n}} \vx\]
\end{defn}

\subsection{Special Subspaces}
\begin{defn}
Let $L:\R^n \rightarrow \R^m$ be a linear mapping. The \textbf{kernel} is defined by
\[\text{ker}(L) = \{ \vx \in \R^n | L(\vx) = \vec{0}\}\]
The set of all vectors in $\R^n$ (domain) where when $L$ is applied, becomes the zero vector.
\end{defn}

\begin{thrm}
Let $L:\R^n \rightarrow \R^m$ be a linear mapping. Then $L(\vec{0}) = \vec{0}$.
\end{thrm}

\begin{thrm}
Let $L: \R^n \rightarrow \R^m$ be a linear mapping. Then $\text{ker}(L)$ is a subspace of $\R^n$.
\end{thrm}

\begin{defn}
Let $L: \R^n \rightarrow \R^m$ be a linear mapping. The \textbf{range} is
\[R(L) = \{ L(\vx) \in \R^m | \vx \in \R^n\}\]
The set of all vectors in the codomain where $L(\vx)$ is defined.
\end{defn}
\todo{Proofs with these, pg 69}

\begin{thrm}
Let $L: \R^n \rightarrow \R^m$ be a linear mapping. Then $R(L)$ is a subspace of $\R^m$.
\end{thrm}

\subsubsection{Four Fundamental Subspaces of a Matrix}

\begin{thrm}
Let $L: \R^n\rightarrow\R^m$ be a linear mapping and let $A = \lbrack L \rbrack$ be the standard matrix of $L$. Then, $\vx \in \text{ker}(L)$ if and only if $A\vx = \vec{0}$.
\end{thrm}

\begin{defn}
Let $A$ be an $m \times n$ matrix. The set of all $\vx \in \R^n$ such that $A\vx = \vec{0}$ is called the \textbf{nullspace} of $A$. We write
\[\text{Null}(A) = \{\vx \in \R^n | A\vx = \vec{0}\}\]
\end{defn}

\begin{thrm}
Let $A$ be an $m \times n$ matrix. A consistent system of linear equations $A\vx = \vec{b}$ has a unique solution if and only if $\text{Null}(A) = \{\vec{0}\}$.
\end{thrm}

\begin{thrm}
Let $L:\R^n \rightarrow \R^m$ be a linear mpping with standard matrix $\lbrack L \rbrack = A = \lbrack \vec{a}_{1} \cdots \vec{a}_{n} \rbrack$. Then
\[R(L) = \spn\{\vec{a}_{1},\dots,\vec{a}_{n}\}\]
\end{thrm}

\begin{defn}
Let $A = \lbrack \vec{a}_{1} \cdots \vec{a}_n \rbrack$. The \textbf{columnspace} of $A$ s the subspace of $\R^m$ defined by
\[\text{Col}(A) = \spn\{\vec{a}_1,\dots,\vec{a}_n\} = \{A\vx \in \R^m | \vx \in \R^n\}\]
It is the span of a set created from the columns of $A$.
\end{defn}

\begin{thrm}
Let $A$ be an $m \times n$ matrix. Then $\text{Col}(A) = \R^m$ if and only if $\text{rank}(A) = m$.
\end{thrm}

\begin{defn}
Let $A$ be an $m \times n$ matrix. The \textbf{rowspace} of $A$ is the subspace of $\R^n$ defined by
\[\text{Row}(A) = \{A^T \vx \in \R^n | \vx \in \R^m\}\]
It is the span of the rows of $A$.
\end{defn}

\begin{defn}
Let $A$ be an $m \times n$ matrix. The \textbf{left nullspace} of $A$ is the subspace of $\R^m$ defined by
\[\text{Null}(A^T) = \{\vx \in \R^m | A^T \vx = \vec{0}\}\]
It is the nullspace of the transpose of $A$.
\end{defn}

\begin{thrm}
Let $A$ be an $m \times n$ matrix. If $\vec{a} \in \text{Row}(A)$ and $\vx \in \text{Null}(A)$, then $\vec{a} \cdot \vx = 0$.
\end{thrm}

\begin{thrm}
Let $A$ be an $m \times n$ matrix. If $\vec{a} \in \text{Col}(A)$ and $\vx \in \text{Null}(A^T)$, then $\vec{a} \cdot \vx = 0$.
\end{thrm}

\subsection{Operations on Linear Mapping}

\begin{defn}
\textbf{Addition \& Scalar Multiplication}:
\[(L + M)(\vx) = L(\vx) + M(\vx)\]
\[(cL)(\vx) = c L(\vx)\]
\end{defn}

\begin{note}
Two linear mappings $L$ and $M$ are equal if and only if they have the same domain, same range, and $L(\vx) = M(\vx)$ for all $\vx$ in the domain.
\end{note}

\begin{thrm}
Let $L,M,N \in \mathbb{L}$ and let $c_{1},c_{2}$ be real scalars. Then
\begin{itemize}
\item $L + M \in \mathbb{L}$
\item $(L + M) + N = L + (M + N)$
\item $L + M = M + L$
\item There exists a linear mapping $O: \R^n \rightarrow \R^m$ such that $L + O = L$. This means $O(\vx) = \vec{0}$ for all $\vx \in \R^n$.
\item There exists $(-L)$ such that $L + (-L) = O$.
\item $c_{1} L \in \mathbb{L}$
\item $c_{1} (c_{2} L) = (c_{1} c_{2}) L$
\item $(c_{1} + c_{2}) L = c_{1} L + c_2 L$
\item $c_{1} (L + M) = c_1 L + c_1 M$
\item $1L = L$
\end{itemize}
\end{thrm}

\begin{thrm}
Let $L: \R^n \rightarrow \R^m$ and $M: \R^n \rightarrow \R^m$ be linear mapping and let $c \in \R$. Then
\[\lbrack L + M \rbrack = \lbrack L \rbrack + \lbrack M \rbrack\]
\[\lbrack cL \rbrack = c \lbrack L \rbrack\]
\end{thrm}

\begin{defn}
Let $L:\R^n \rightarrow \R^m$ and $M:\R^n \rightarrow \R^m$ be linear mappings. Then $M$ composed of $L$ is the function $M \circ L: \R^n \rightarrow \R^p$ defined by
\[(M \circ L)(\vx) = M(L(\vx))\]
\end{defn}

\begin{thrm}
Let $L: \R^n \rightarrow \R^m$ and $M: \R^n \rightarrow \R^m$ be linear mappings. then $M \circ L$ is a linear mapping and
\[\lbrack M \circ L \rbrack = \lbrack M \rbrack \lbrack L \rbrack\]
\end{thrm}

\section{Vector Spaces}

\subsection{Vector Spaces}

\begin{defn}
Let $\mathbb{V}$ be a set. The elements of $\mathbb{V}$ are vectors denoted as $\vx$. $\mathbb{V}$ is called a \textbf{vector space over $\R$} if there is an operation of addition and scalar multiplication such that for any $\vx,\vy,\vv \in \mathbb{V}$ and $a,b \in \R$,
\begin{enumerate}
\item $\vx + \vy \in \mathbb{V}$
\item $(\vx + \vy) + \vv = \vx + (\vy + \vv)$
\item $\vx + \vy = \vy + \vx$
\item The zero vector exists in $\mathbb{V}$, $\vx + \vec{0} = \vx$
\item For each $\vx \in \mathbb{V}$, there exists $-\vx$ such that $\vx + (- \vx) = \vec{0}$, known as the \textbf{additive inverse}
\item $a\vx \in \mathbb{V}$
\item $a (b\vx) = (ab)\vx$
\item $(a+b)\vx = a\vx + b\vx$
\item $a(\vx + \vy) = a\vx + a\vy$
\item $1\vx = \vx$
\end{enumerate}
\end{defn}

\begin{exmp}
Is the empty set a vector space?
\begin{sol}
No. It does not contain $\vec{0}$ even though the other statements are vacuously true.
\end{sol}
\end{exmp}

\begin{exmp}
Let $\mathbb{V} = \{\vec{0}\}$ and define addition by $\vec{0} + \vec{0} = \vec{0}$ and scalar multiplication by $c \vec{0} = \vec{0}$. Show that $\mathbb{V}$ is a vector space.
\begin{sol}
Must show that it satisfies all ten axioms.
\begin{enumerate}
\item The only element in $\mathbb{V}$ is $\vec{0}$ and $\vec{0} + \vec{0} = \vec{0} \in \mathbb{V}$
\item $(\vec{0} + \vec{0}) + \vec{0} = \vec{0} + (\vec{0} + \vec{0})$
\item $\vec{0} + \vec{0} = \vec{0} = \vec{0} + \vec{0}$
\item $\vec{0} + \vec{0} = \vec{0}$ so the zero vector is in the set.
\item Additive inverse of $\vec{0}$ is $\vec{0}$.
\item $a\vec{0} = \vec{0} \in \mathbb{V}$
\item $a(b\vec{0}) = a\vec{0} = \vec{0} = (ab)\vec{0}$
\item $(a+b)\vec{0} = \vec{0} = \vec{0} + \vec{0} = a\vec{0} + b\vec{0}$
\item $a(\vec{0} + \vec{0}) = a\vec{0} = \vec{0} = \vec{0} + \vec{0} = a\vec{0} + a\vec{0}$
\item $1\, \vec{0} = \vec{0}$
\end{enumerate}
\end{sol}
\end{exmp}

\begin{exmp}
Let $\mathbb{S}  = \{x \in \R | x > 0\}$. Define addition in $\mathbb{S}$ by $x \oplus y = xy$ and define sclar multiplication by $c \odot x = x^c$ for all $x,y \in \mathbb{S}$ and all $c \in \R$. Prove that $\mathbb{S}$ is a vector space under these operations.


\begin{sol}
Must should that $\mathbb{S}$ satisfies all ten vector space axioms. For any $x,y,z \in \mathbb{S}$ and $a,b \in \R$ we have
\begin{enumerate}
\item $x \oplus y = xy > 0 $ since $ x > 0$ and $y > 0$, hence $x \oplus y \in \mathbb{S}$
\item $(x \oplus y) \oplus z = (xy) \oplus z = (xy)z  x(yz) = x \oplus (yz) = x \oplus (y \oplus z)$
\item $x \oplus y = xy = yx = y \oplus x$
\item The zero vector is $1$ because $1 \in \mathbb{S}$ and $x \oplus 1 = x1 = x$
\item $\f{1}{x}$ is the additive inverse of $x$ since $\f{1}{x} \in \mathbb{S}$ and $\f{1}{x} \oplus x = 1$.
\item $a \odot x = x^n > 0$ since $x > 0$ so $a \odot x \in \mathbb{S}$.
\item $a \odot (b \odot x) = a \odot x^b = (x^b)^a = x^{ab} = (ab) \odot x$
\item $(a+b) \odot x = x^{a+b} = x^a x^b = x^a \oplus x^b = a \odot x \oplus b \odot x$
\item $a \odot (x \oplus y) = a \odot (xy) = (xy)^a = x^a y^a = x^a \oplus y^a = a \odot x \oplus a \odot y$
\item $1x = x^1 = x$
\end{enumerate}
Therefore $\mathbb{S}$ is a vector space.
\end{sol}
\end{exmp}

\begin{exmp}
$\mathbb{V} = \{\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} | x_1,x_2 \in \R\}$ with standard scalar multiplication, but addition defined by
\[\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} + \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} = \begin{bmatrix} x_1 + y_2 \\ y_1 + x_2 \end{bmatrix}\]
\begin{sol}
This is not a vector space because $\begin{bmatrix} 1 \\ 0 \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ but $\begin{bmatrix} 1 \\ 0 \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$\\
This does not satisfy V3.
\end{sol}
\end{exmp}

\begin{exmp}
Show that the set $\Z^2$ = $\left\{\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} | x_1,x_2 \in \Z \right\}$ is not a vector space under standard addition and scalar multiplication of vectors.
\begin{sol}
Observe that $\begin{bmatrix} 1 \\ 2 \end{bmatrix} \in \Z^2$, but $\sqrt{2} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \begin{bmatrix} \sqrt{2} \\ 2 \sqrt{2} \end{bmatrix} \not\in \Z^2$. Hence this does not satisfy V6 and is not a vector space.
\end{sol}
\end{exmp}

\begin{thrm}
Let $\mathbb{V}$ be a vector space with addition defined by $\vx + \vy$ and scalar multiplication defined by $c \vx$ for all $\vx,\vy \in \mathbb{V}$, and $c \in \R$, Then
\begin{itemize}
\item $0 \vec{x} = \vec{0}$ for all $\vx \in \mathbb{V}$
\item $- \vx = (-1)\vx$ for all $\vx \in \mathbb{V}$
\end{itemize}
\end{thrm}

\subsubsection{Subspaces}

\begin{defn}
Let $\mathbb{V}$ be a vector space. If $\mathbb{S}$ is a subset of $\mathbb{V}$ and $\mathbb{S}$ is a vector space under the same operations as $\mathbb{V}$, then $\mathbb{S}$ is called a \textbf{subspace} of $\mathbb{V}$.
\end{defn}

\begin{thrm}
Let $\mathbb{S}$ be a non-empty subset of $\mathbb{V}$. If $\vx + \vy \in \mathbb{S}$ and $c\vx \in \mathbb{S}$ for all $\vx,\vy \in \mathbb{S}$, and $c \in \R$ under the operations of $\mathbb{V}$, then $\mathbb{S}$ is a subspace of $\mathbb{V}$
\end{thrm}

\begin{exmp}
Is $\W = \{p(x) \in P_2(\R)|p(2) = 0\}$ a subspace of $P_2(\R)$?
\begin{sol}
In $P_2(\R)$ the zero vector is the polynomial that satisfies $z(x) = 0$ for all $x$. Hence $z(x) \in \W$ since $z(2) = 0$. Therefore $\W$ is non-empty.\n
Let $p(x),q(x) \in \W$. Then $p(2) = 0, q(2) = 0, (p+q)(2) = p(2) + q(2) = 0 + 0 = 0$. Hence $(p+q) \in \W$ and $\W$ is closed under addition.\n
Similarly, $(cp)(2) = cp(2) = c0 = 0$ for all $c \in \R$ so $(cp) \in \W$. Thus, it is also closed unders calar multiplication. Therefore $\W$ is a subspace of $P_2(\R)$ by the Subspace Test.
\end{sol}
\end{exmp}

\begin{exmp}
IS $T = \{a + bx + cx^2 \in P_3(\R)|a^2 - b^2 = 0\}$ a subspace of $P_2(\R)$?
\begin{sol}
Observe $-1 + 4x \not\in T$. Therefore this is not a subspace.
\end{sol}
\end{exmp}

\subsubsection{Spanning}
\begin{defn}
Let $B = \{\vv_1,\dots,\vv_k\}$ be a set of vectors in a vector space $\V$. Then we define the \textbf{span} of $B$ by
\[\spn(B) = \{c_1 \vv_1 + \cdots + c_k \vv_k | c_1,\dots,c_k \in \R\}\]
$\spn B$ is \textbf{spanned} by $B$ and $B$ is a \textbf{spanning set} for $\spn B$.
\end{defn}

\begin{thrm}
If
If $B = \{\vv_1,\dots,\vv_k\}$ is a span of vectors in a vector space $\V$, then $\spn B$ is a subspace of $\V$.
\end{thrm}

\begin{thrm}
Let $\V$ be a vector space and $\vv_1,\dots,vv_k \in \V$. Then $v_i \in \spn \{\vv_1,\dots,\vv_{i-1},\vv_{i+1},\dots,\vv_k\}$.
\end{thrm}

\begin{exmp}
Determine if $p(x) = 3 - 4x + 2x^2$ is in $\spn\{1+2x,1-x+3x^2,2-x+x^2\}$ in $P_2(\R)$.
\begin{sol}
Must determine if there exists coefficients such that
\begin{align*}
3 - 2x + 2x^2 &= c_{1}(1+2x) + c_{2} (1-x+3x^2) + c_{3}(2-x+x^2) \\
&= (c_{1} + c_{2} + 3 c_{3}) + (2c_{1} - c_{2} - c_{3}) x + (3c_{2} + c_{3})x^2
\end{align*}
Collect like coefficients
\end{sol}
\begin{align*}
c_{1} + c_{2} + 3 c_{3} &= 3 \\
2c_{1} - c_{2} - c_{3} &= 4 \\
3c_{2} + c_{3} &= 2 \\
\end{align*}
\[\begin{bmatrix}[ccc|c] 1 & 1 & 2 & 3 \\ 3 & -1 & -1 & 4 \\  & 3 & 1 & 2 \end{bmatrix} \sim \begin{bmatrix}[ccc|c] 1 & 0 & 0 & -1 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 2 \end{bmatrix}\]
Therefore $p(x) \in \spn\{1+2x,1-x+2x^2,2-x+x^2\}$
\end{exmp}

\begin{defn}
A set of vectors $\{\vv_1,\dots,\vv_k\}$ in a vector space $\V$ is \textbf{linearly dependent} if there exists at least one non-zero coefficient that satisfies
\[\vec{0} = c_1\vv_1 + \cdots + c_k\vv_k\]
The set is \textbf{linearly independent} if the only solution is the trivial solution.
\end{defn}

\begin{thrm}
A set of vectors $\{\vv_1,\dots,\vv_k\}$ in a vector space $\V$ is linearly dependent if and only if there exists $1 \leq i \leq k$ such that
\[\vv_i \in \spn \{\vv_1,\dots,\vv_{i-1},\vv_{i+1},\vv_k\}\]
\end{thrm}

\begin{thrm}
A set of vectors $\{\vv_1,\dots,\vv_k\}$ in a vector space $\V$ which contains the zero vector is linearly dependent.
\end{thrm}

\begin{exmp}
Determine if the set $\{1 + x + 2x^2,x-x^2,-2x^2\}$ is linearly independent.
\begin{sol}
A set is linear independnet if and only if the only solution to
\[0 = c_{1} (1 + x + 2x^2) + c_{}(x-x^2) + c_{3} (-2 + x^2)\]
is $c_1 = c_2 = c_3 = 0$. Rearranging,
\[(c_1 - 2c_3) + (c_1 + c_2) x + (2c_1 - c_2 + c_3)x^2 = 0\]
Solve the homogeneous system,
\[\begin{bmatrix} 1 & 0 & -2 \\ 1 & 1 & 0 \\ 2 & -1 & 1 \end{bmatrix} \sim \identity \]
This system has a unique solution, thus the set is linearly independent.
\end{sol}
\end{exmp}

\begin{exmp}
Determine if $\{1 + 2x + x^2,3 + 3x + 2x^2, 5 + x + 3x^2\}$ is linearly independent in $P_2(\R)$.
\begin{sol}
\begin{align*}
0 &= c_{1}(1 + 2x + x^2) + c_{2}(3 + 3x + 2x^2) + c_{3} (5 + x + 3x^2)\\
&= (c_{1} + 3c_{2} + 5 c_{3}) + (2c_{1} + 3c_2 + c_3) x + (c_1 + 2c_2 + 2c_3)x^2
\end{align*}
\[\begin{bmatrix} 1 & 3 & 5 \\ 2 & 3 & 1 \\ 1 & 2 & 2 \end{bmatrix} \sim \begin{bmatrix} 1 & 0 & -4 \\ 0 & 1 & 3 \\ 0 & 0 & 0 \end{bmatrix}\]
Since there are infinitely many solutions, the system is linearly dependent.
\end{sol}
\end{exmp}

\subsection{Bases and Dimension}

\begin{defn}
Let $\V$ be a vector space. The set $B$ is called a basis for $\V$ if $B$ is linearly independent spanning set for $\V$.
\end{defn}

\begin{exmp}
Find the standard basis for $P_n(\R)$.
\begin{sol}
Every vector in $P_n(\R)$ has teh form
\[P(x) = a_0 + a_1x + \cdots + a_n x^n\]
Thus the set $\{1,x,\dots,x^n\}$ spans $P_n(\R)$. In addition
\[0 + 0x + \cdots + 0x^n = a_0 + a_1 x + \cdots + a_n x^n\]
By equating like powers of $x$, the only solution is the trivial solution. Therefore $\{1,x,\dots,x^n\}$ is a linearly independent spanning set for $P_n(\R)$ and is its standard basis.
\end{sol}
\end{exmp}

\begin{exmp}
Prove that $B = \{1,(1-x),(1-x)^2\}$ is a basis for $P_2(\R)$.
\begin{sol}
Let $p(x) = a + bx + cx^2$.
\begin{align*}
a + bx + cx^2 &= c_{1} + c_{2}(1-x) + c_{3} (1-x)^2 \\
&= (c_0 + c_1 + c_2) + (-c_1 - 2c_2)x + c_2 x^2
\end{align*}
Therefore,
\begin{align*}
c_0 + c_1 + c_2 &=a\\
-c_1 - 2c_2 &=b\\
c_2 &=c
\end{align*}
If the system is row reduced, we . Thus $B$ is a linearly independent spanning set for $P_2(\R)$.
\end{sol}
\end{exmp}

\begin{exmp}
Find a basis for the space of $M_{2\times2}(\R)$ defined by
\[S = \left\{\begin{bmatrix} a & b \\ 0 & c \end{bmatrix} | a-b = 2c\right\}\]
\begin{sol}
Every vector has the form
\[\begin{bmatrix} a & b \\ 0 & c \end{bmatrix} = \begin{bmatrix} a & a-2c \\ 0 & c \end{bmatrix} = a \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} + c \begin{bmatrix} 0 & -2 \\ 0 & 1 \end{bmatrix}\]
Thus $B = \left\{\begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}, \begin{bmatrix} 0 & -2 \\ 0 & 1 \end{bmatrix}\right\}$ spans $S$, and is clearly linearly independent. Therefore it is a basis for $S$.
\end{sol}
\end{exmp}

\begin{thrm}
Let $B = \{\vv_1,\dots,\vv_n\}$ be a basis for a vector space $\V$ and let $C = \{\vw_1,\dots,\vw_k\}$ be a set in $\V$. If $k > n$ (rank) then $C$ is linearly dependent.

\begin{proof}
Consider $0 = c_1\vw_1 + \cdots + c_k \vw_k$. \n
Since $B$ is a basis for $\V$, we can write every vector $\vw$ as a linear combination of the vectors in $B$.
\[w_i = a_{i1}\vv_1 + \cdots + a_{in} \vv_n,\, \text{ for } 1 \leq i \leq k\]
Substituting,
\[ 0 = (c_1  a_{11} + \cdots + c_k a_{k1})\vv_1 + \cdots + (c_1 a_{1n} + \cdots + c_k a_{kn}) \vv_n\]
Since $B$ is a basis, it is linearly independent, and the only solution is when
\[c_1 a_{11} + \cdots + c_k a_{k1} = 0\]
\[\vdots\]
\[c_1 a_{1n} + \cdots + c_k a_{kn} = 0\]
Since $k > n$, the system has infinitely many solutions by the system rank theorem, so the equation has infinitely many solutions, and hence $C$ is linearly independent.
\end{proof}
\end{thrm}

\begin{thrm}
If $B = \{\vv_1,\dots,\vv_n\}$ and $C = \{ \vw_1,\dots,\vw_k\}$ are bases for the vector space $\V$, then $k = n$.

\begin{proof}
Since $B$ is a basis and $C$ is linearly independent, $k \leq n$ by the previous theorem. Similary, since $C$ is a basis and $B$ is linearly independent, $n \neq k$. Hence $n = k$.
\end{proof}
\end{thrm}

\begin{defn}
Let $\{\vv_1,\dots,\vv_k\}$ be a basis for the vector space $\V$. The \textbf{dimension} of $\V$ is $n$ (number of elements in the basis) and we write
\[\text{dim}(V) = n\]
If $V = \{\vec{0}\}$ then $\text{dim}(V) = 0$. If $\V$ does not have a basis with a finite number of vectors in it, then $\V$ is \textbf{infinite dimensional}.
\end{defn}

\begin{exmp}
Some common dimensions
\begin{itemize}
\item The dimension of $\R^n$ is $n$.
\item The dimension of $P_n(\R)$ is $n+1$.
\item The dimension if $M_{m \times n}(\R)$ is $mn$
\item The vector space $P(\R)$ of all polynomials with eral coefficients is infinite dimensional since the basis is $\{1,x,x^2,\dots\}$.
\end{itemize}
\end{exmp}

\begin{exmp}
The basis of $B = \left\{\begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}, \begin{bmatrix} 0 & -2 \\ 0 & 1 \end{bmatrix}\right\}$ is $2$ because there are two elements in the set.
\end{exmp}

\begin{thrm}
Let $\V$ be an n-dimensional vector space. Then

\begin{enumerate}
\item A set of more than $n$ vectors in $\V$ must be inearly dependent.
\item A set of fewer than $n$ vectors in $\V$ cannot span $\V$.
\item A set of $n$ vectors in $\V$ is linearly independent if and only if it spans $\V$.
\end{enumerate}
\end{thrm}

\begin{thrm}
If $\V$ is an n-dimensional vector space and $\{\vv_1,\dots,\vv_k\}$ is a linearly independent set in $\V$ with $k < n$, then there exists vectors $\vw_{k+1},\dots,\vw_{n}$ in $\V$ such that $\{\vv_1,\dots,\vv_k,\vw_{k+1},\dots,\vw_n\}$ is a basis for $\V$.

\begin{proof}
By Theorem 4.2.3, $\{\vv_1,\dots,\vv_k\}$ does not span $\V$.Let $\vw_{k+1}$ be a vector in $\V$ such that $\vw_{k+1} \not \in \spn\{\vv_1,\dots,\vv_k\}$. If $k+1 = n$, then by Theorem 4.2.3, $\{\vv_1,\dots,\vv_k,\vw_{k+1}\}$ is a basis. Else, repeat the procedure until it is true, and the set will be
\[\{\vv_1,\dots,\vv_k,\vw_{k+1},\dots,\vw_n\}\]
\end{proof}
\end{thrm}

\begin{exmp}
Find a basis for the hyperplane with the equation $2x_1 + x_2 - x_3 - x_4 = 0$ and extend the basis to be a basis for $\R^4$.

\begin{sol}
Pick three vectors that are linearly independent and satisfy the hyperplane.\\
\[\vv_1 = \begin{bmatrix} 0 \\ 2 \\ 1 \\ 1\end{bmatrix}, \vv_2 = \begin{bmatrix} 0 \\ 0 \\ -1 \\ 1 \end{bmatrix}, \vv_3 = \begin{bmatrix} 1 \\ 0 \\ 1 \\ 1 \end{bmatrix}\]
Clearly the set $\{\vv_1,\vv_2,\vv_3\}$ is linearly independent, so it is a basis by Theorem 4.2.3 since the dimension of a hyperplanein $\R^4$ is $3$. To extend the basis to $\R^4$, we pick
\[\vec{n} = \begin{bmatrix} 2 \\ 1 \\ -1 \\ -1 \end{bmatrix}\]
(Observe that this vector is not spanned by the hyperplane). By Theorem 4.2.4, $\{\vv_1,\vv_2,\vv_3,\vec{n}\}$ is a basis for $\R^4$.
\end{sol}
\end{exmp}

\begin{cor}
If $S$ is a subspace of a finite dimensional vector space $\V$, then $\text{dim}(S) \leq \text{dim}(V)$.
\end{cor}

\subsection{Coordinates}

\begin{thrm}
If $B = \{\vv_1,\dots,\vv_n\}$ is a basis for a vector space $\V$, then every vector $\vv \in \V$ can be represented as a \textbf{unique} linear combination of $\vv_1,\dots,\vv_n$.
\begin{proof}
Since $B$ is a basis, it a spanning set. Ths for every vector $\vv \in \V$ there exists constants such that
\[c_1 \vv_1 + \cdots + c_n \vv_n = \vv\]
Assume that there also exists constants such that $d_1 \vv_1 + \cdots + d_n \vv_n = \vv$. Then
\[c_1 \vv_1 + \cdots + c_n \vv_n = d_1 \vv_1 + \cdots + d_n \vv_n = \vv\]
\[(c_1 - d_1) \vv_1 + \cdots + (c_n - d_n)\vv_n = \vec{0}\]
But this implies $c_i = d_i$ for all $1 \leq i \leq n$ since $B$ is linearly independent. Thus there exists only one linear combination of the vectors in $B$ that equals $\vv$.
\end{proof}
\end{thrm}

\begin{defn}
Let $\V$ be a vector space with basis $B = \{\vv_1,\dots,\vv_n\}$. For any $\vv \in \V$, the \textbf{coordinate vector} of $\vv$ with respect to $B$ is
\[\lbrack \vv \rbrack_B = \begin{bmatrix} b_{1} \\ \vdots \\ b_{n} \end{bmatrix}\]
where $\vv = b_1\vv_1 + \cdots + b_n\vv_n$.
\end{defn}

\begin{exmp}
Given that $B = \left\{\begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix}, \begin{bmatrix} 0 & 1 \\ 0 & 1 \end{bmatrix}, \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}\right\}$ is a basis for a subspace $S$ of $M_{2 \times 2}(\R)$ and $\lbrack A \rbrack_B = \begin{bmatrix} 2 \\ -1 \\ 3 \end{bmatrix}$, what is $A$?
\begin{sol}
We have $A = 2 \begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix} + (-1) \begin{bmatrix} 0 & 1 \\ 0 & 1 \end{bmatrix} + 3 \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix} = \begin{bmatrix} 5 & 7 \\ 6 & 6 \end{bmatrix}$
\end{sol}
\end{exmp}

\begin{exmp}
Consider the basis $B = \{1,(x-1),(x-1)^2\}$ for $P_2(\R)$. Find the B coordinate vectors of $p(x) = 3 - 5x + 4x^2$ and $q(x) = x$.
\begin{sol}
Must find constants such that
\begin{align*}
3 - 5x + 4x^2 &= c_{1} + c_{2} (x-1) + c_{3} (x-1)^2 \\
&= (c_1 - c_2 + c_3) + (c_2 - 2c_3)x + c_3 x^2
\end{align*}
Similarly, we need to find
\begin{align*}
x &= d_1 + d_2(x-1) + d_3 (x-1)^2\\
&= (d_1 - d_2 + d_3) + (d_2 - 2d_3)x + d_3 x^2
\end{align*}
The coefficients for both these augmented matrices are the same, so a double augmented matrix can be created.
\[\begin{bmatrix}[ccc|c|c] 1 & -1 & 1 & 3 & 0 \\ 0 & 1 & -2 & -5 & 1 \\ 0 & 0 & 1 & 4 & 0 \end{bmatrix} \sim \begin{bmatrix}[ccc|c|c] 1 & 0 & 0 & 2 & 1 \\ 0 & 1 & 0 & 3 & 1 \\ 0 & 0 & 1 & 4 & 0 \end{bmatrix}\]
Therefore $\lbrack 3 - 5x + 4x^2 \rbrack_B = \begin{bmatrix} 2 \\ 3 \\ 4 \end{bmatrix}$, and $\lbrack x \rbrack_B = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}$.
\end{sol}
\end{exmp}

\begin{thrm}
If $\V$ is a vector space with $B = \{\vv_1,\dots,\vv_n\}$, then for any $\vv,\vw \in \V$, and $s,t \in \R$, we have
\[\lbrack s \vv + t \vw \rbrack_B = s \lbrack \vv \rbrack_b + t \lbrack \vw \rbrack_B\]
\begin{proof}
Let $\vv = b_1 \vv_1 + \cdots + b_n \vv_n$ and $w = c_1 \vv_1 + \cdots c_n \vv_n$. Then we have
\[s\vv + t \vw = (sb_1 + tc_1) \vv_1 + \cdots + (sb_n + tc_n) \vv_n\]
Therefore,
\[\lbrack s \vv + t \vw \rbrack_B = \begin{bmatrix} sb_1 + tc_1 \\ \vdots \\ sb_n + tc_n \end{bmatrix} = s \begin{bmatrix} b_1 \\ \vdots \\b_n \end{bmatrix} + t \begin{bmatrix} c_1 \\ \vdots \\c_n \end{bmatrix} = s \lbrack \vv \rbrack_B + t \lbrack \vw \rbrack_B\]
\end{proof}
\end{thrm}

\subsubsection{Change of Coordinates}

\begin{exmp}
Let $B$ be any basis for $\R^3$ and let $\vx \in \R^3$.
\[\vx = x_1 \ve_1 + x_2 \ve_2 + x_3 \ve_3 \]
If we find the coordinates of the standard basis vectors with respect to the basis $B$, then calculating $\lbrack x \rbrack_B$ will be easy
\begin{align*}
\left\lbrack \begin{bmatrix} x_1 \\ x_2 \\x_3 \end{bmatrix} \right\rbrack_B &= \lbrack x_1 \ve_1 + x_2 \ve_2 + x_3 \ve_3 \rbrack_B \\
&= x_1 \lbrack \ve_1 \rbrack_B + x_2 \lbrack \ve_2 \rbrack_B + x_2 \lbrack \ve_2 \rbrack_B \\
&= \left \lbrack \lbrack \ve_1 \rbrack_B + \lbrack \ve_2 \rbrack_B + \lbrack \ve_3 \rbrack_B \right \rbrack \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}
\end{align*}
We call $_BP_S = \lbrack \ve_1 \rbrack_B + \lbrack \ve_2 \rbrack_B + \lbrack \ve_3 \rbrack_B$ the change of coordinates matrix from the standard basis $S$ to the basis $B$.
\end{exmp}

\begin{exmp}
Let $B = \left\{\begin{bmatrix} 1 \\ 3 \\ -1 \end{bmatrix}, \begin{bmatrix} 2 \\ 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 3 \\ 4 \\ 1 \end{bmatrix}\right\} = \{\vec{b}_1, \vec{b}_2, \vec{b}_3\}$.\n
Find $\lbrack \vx \rbrack_B$ for any $\vx \in \R^3$.
\begin{sol}
\[\ve_1 = c_1 \vb_1 + c_2 \vb_2 + c_3 \vb_3\]
\[\ve_2 = d_1 \vb_1 + d_2 \vb_2 + d_3 \vb_3\]
\[\ve_3 = f_1 \vb_1 + f_2 \vb_2 + f_3 \vb_3\]
Use a triple augmented matrix to find the coefficients
\[\begin{bmatrix}[ccc|ccc] 1 & 2 & 3 & 1 & 0 & 0 \\ 3 & 1 & 4 & 0 & 1 & 0 \\ -1 & 1 & 1 & 0 & 0 & 1 \end{bmatrix} \sim \begin{bmatrix}[ccc|ccc] 1 & 0 & 0 & \f 3 5 & - \f 1 5 & -1 \\ 0 & 1 & 0 & \f 7 5 & - \f 4 5 & -1 \\ 0 & 0 & 1 & - \f 4 5 & \f 3 5 & 1 \end{bmatrix}\]
\[\lbrack \vx \rbrack_B = _B P_S = \left \lbrack \lbrack \ve_1 \rbrack_B + \lbrack \ve_2 \rbrack_B + \lbrack \ve_3 \rbrack_B \right \rbrack = \begin{bmatrix} \f 3 5 & - \f 1 5 & 1 \\ \f 7 5 & - \f 4 5 & -1 \\ - \f 4 5 & \f 3 5 & 1 \end{bmatrix}\]
\[\lbrack \vx \rbrack_B = _B P_S \vx = \begin{bmatrix} \f 3 5 x_1 - \f 1 5 x_2 + x_3\\ \f 7 5 x_1 - \f 4 5 x_2 - x_3 \\ - \f 4 5 x_1 + \f 3 5 x_2 + x_3\end{bmatrix}\]
\end{sol}
\end{exmp}

\begin{defn}
Let $B = \{\vv+1,\dots,\vv_n\}$ and $C$ both be basis for a vector space $\V$. The \textbf{change of coordinate matrix} from B-coordinate to C-coordinate is defined by
\[_CP_B = \lbrack \lbrack \vv_1 \rbrack_C \cdots \lbrack \vv_n \rbrack_C \rbrack\]
and $\forall \vx \in \V$, we have
\[\lbrack \vx \rbrack_C = _CP_B \lbrack \vx \rbrack_B\]
\end{defn}

\begin{exmp}
Let $B = \{1 + 3x,2+x\}$ and $C = \{-1+x,5-4x\}$ both be basis of $P_1(\R)$. Find $_CP_B$ and $_BP_C$.

\begin{sol}
To find $_CP_B$, must find the C-coordinate of the vectors in $B$,
\[1 + 3x = c_1 (-1 + x) + c_2 (5-4x)\]
\[2+x = d_1(-1+x) + d_2(5-4x)\]
Create a double augmented matrix and row reduce
\[\begin{bmatrix}[cc|cc] -1 & 5 & 1 & 2 \\ 1 & -4 & 3 & 1 \end{bmatrix} \sim \begin{bmatrix}[cc|cc] 1 & 0 & 19 & 13 \\ 0 & 1 & 4 & 3 \end{bmatrix}\]
Therefore $_CP_B = \begin{bmatrix} 19 & 13 \\ 4 & 3 \end{bmatrix}$. To find $_BP_C$,
\[-1 + x = c_1 (1 + 3x) + c_2 (2 + x)\]
\[5 - 4x = d_1(1+3x) + d_2 (2+x)\]
Creating a double augmented matrix and row reducing gives
\[\begin{bmatrix}[cc|cc] 1 & 2 & -1 & 5 \\ 3 & 1 & 1 & -4 \end{bmatrix} \sim \begin{bmatrix}[cc|cc] 1 & 0 & \f 3 5 & - \f{13}{5} \\ 0 & 1 & - \f 4 5 & \f{19}{15} \end{bmatrix}\]
Therefore $_BP_C = \begin{bmatrix} \f 3 5 & -\f{13}{5} \\ - \f 4 5 & \f{19}{5} \end{bmatrix}$.
\end{sol}
\end{exmp}

\begin{thrm}
If $B$ and $C$ are bases for an n-dimensional vector space $\V$, then the change of coordinate matrices $_CP_B$ and $_BP_C$ satisfy
\[_CP_B\,_BP_C = I = \,_BP_C\,_CP_B\]
\end{thrm}

\section{Inverses and Determinants}

\section{Diagonalization}


\todo{Alot of stuff here}
\subsubsection*{3/27/15}

\begin{exmp}
Consider $A$ = $\begin{bmatrix} 3 & 6 & 7 \\ 3 & 3 & 7 \\ 5 & 6 & 5 \end{bmatrix}$. with corresponding eigenvectors $\vv_{1} = \begin{bmatrix} 1 \\ -2 \\ 1 \end{bmatrix}$ since
\[\begin{bmatrix} 3 & 6 & 7 \\ 3 & 3 & 7 \\ 5 & 6 & 5 \end{bmatrix} \begin{bmatrix} 1 \\ -2 \\ 1 \end{bmatrix} = \begin{bmatrix} -2 \\ 4 \\ -2 \end{bmatrix} = 2 \begin{bmatrix} 1 \\ -2 \\ 1 \end{bmatrix}\]
\end{exmp}

\begin{exmp}
$\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$ is not an eigenvector of $A$ because
\[\begin{bmatrix} 3 & 6 & 7 \\ 3 & 3 & 7 \\ 5 & 6 & 5 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 16 \\ 13 \\ 16 \end{bmatrix}\]
and this is not a scalar multiple of $\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$.
\end{exmp}

\begin{exmp}
Is $\lambda=2$ an eigenvalue of $A$? Is there a non-zero vector $\vv$ such that $A\vv = 2 \vv$?

\begin{sol}
\[\begin{bmatrix} 3 & 6 & 7 \\ 3 & 3 & 7 \\ 5 & 6 & 5 \end{bmatrix} \begin{bmatrix} v_{1} \\ v_{2} \\ v_{3} \end{bmatrix} = 2 \begin{bmatrix} v_{1} \\ v_{2} \\ v_{3} \end{bmatrix}\]
\[3v_{1} + 6 v_{2} + 7 v_{3} = 2v_{1}\]
\[3v_{1} + 3v_{2} + 7 v_{3} = 2v_{2}\]
\[5v_{1} + 6v_{2} + 5 v_{3} = 2v_{3}\]
so
\[1v_{1} + 6 v_{2} + 7 v_{3} = 0\]
\[3v_{1} + 1v_{2} + 7 v_{3} = 0\]
\[5v_{1} + 6v_{2} + 3 v_{3} = 0\]
Put this into a matrix and row reduce.
\[\begin{bmatrix} 1 & 6 & 7 \\ 3 & 1 & 7 \\ 5 & 6 & 3\end{bmatrix} - \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{bmatrix}\]
The only solution is $\vv = 0$ therefore, there is no non-zero vector that satisfies $A\vv = 2 \vv$ so $\lambda = 2$ is not an eigenvalue.
\end{sol}
\end{exmp}

\begin{exmp}
Is $\lambda = 15$ an eigenvalue of $A$? $A\vv = 15\vv$ exists?

\begin{sol}
\[\begin{bmatrix} 3 & 6 & 7 \\ 3 & 3 & 7 \\ 5 & 6 & 5 \end{bmatrix} \begin{bmatrix} v_{1} \\ v_{2} \\ v_{3} \end{bmatrix} = 15 \begin{bmatrix} v_{1} \\ v_{2} \\ v_{3} \end{bmatrix}\]

\[3v_{1} + 6 v_{2} + 7 v_{3} = 15v_{1}\]
\[3v_{1} + 3v_{2} + 7 v_{3} = 15v_{2}\]
\[5v_{1} + 6v_{2} + 5 v_{3} = 15v_{3}\]
so
\[-12v_{1} + 6 v_{2} + 7 v_{3} = 0\]
\[3v_{1}  -12v_{2} + 7 v_{3} = 0\]
\[5v_{1} + 6v_{2} - 10 v_{3} = 0\]

\[\begin{bmatrix} -12 & 6 & 7 \\ 3 & -12 & 7 \\ 5 & 6 & -10 \end{bmatrix} - \begin{bmatrix} 1 & 0 & -1 \\ 0 & 1 & -\f{5}{6} \\ 0 & 0 & 0 \end{bmatrix}\]
Therefore the solution set is $\vv = t \begin{bmatrix} 1 \\ \f{5}{6} \\ 1 \end{bmatrix}$, $t \neq 0$, and $\lambda = 15$ is an eigenvalue.
\end{sol}
\end{exmp}

Let $A$ be an $n \times n$ matrix. Determine an easy way of determining if a scalar $\lambda$ is an eigenvalue of $A$. If $\lambda$ is an eigenvalue of $A$ with corresponding eigenvector $\vv$, then
\[A\vv = \lambda \vv\]
\[A\vv - \lambda \vv = \vec{0}\]
\[(A - \lambda I) \vv = \vec{0}\]
$\lambda$ is an eigenvalue of $A$ if there exists a non-trivial solution to this equation. This is only possible if $A - \lambda I$ is not invertible. Need to find $\text{det}(A - \lambda I) = 0$.

\begin{exmp}
Find all eigenvalues of $A$ = $\begin{bmatrix} 4 & 2 \\ 1 & 5 \end{bmatrix}$.

\begin{sol}
\[\text{det}(A - \lambda I) = \text{det}\left(\begin{bmatrix} 4 & 2 \\ 1 & 5 \end{bmatrix} - \begin{bmatrix} \lambda & 0 \\ 0 & \lambda \end{bmatrix}\right) = \begin{vmatrix} 4 - \lambda & 2 \\ 1 & 5 - \lambda \end{vmatrix}\]
Use the formula $ad - bc$.
\[= \lambda^2 - 9 \lambda + 18 = (\lambda - 3) (\lambda - 6)\]
Therefore the eigenvalues are $3$ and $6$.
\end{sol}
\end{exmp}

\begin{exmp}
We can find the eigenvectors corresponding to the eigenvalues by solving the homogeneous system $(A - \lambda I) \vv = \vec{0}$ for each eigenvalue. \\\\
For $\lambda_1 = 3$, we have $A - 3I = \begin{bmatrix} 1 & 2 \\ 1 & 2 \end{bmatrix} - \begin{bmatrix} 1 & 2 \\ 0 & 0 \end{bmatrix}$. Thus all eigenvectors corresponding to $\lambda_1$ are $\vv = t \begin{bmatrix} -2 \\ 1 \end{bmatrix}, t \neq 0$. \\\\
For $\lambda_2 = 6$, we have $A - 6I = \begin{bmatrix} -2 & 2 \\ 1 & -1 \end{bmatrix} - \begin{bmatrix} 1 & -1 \\ 0 & 0 \end{bmatrix}$. So all eigenectors corresponding to $\lambda_2$ are $\vv = t \begin{bmatrix} 1 \\ 1 \end{bmatrix}, t \neq 0$.
\end{exmp}

\begin{exmp}
Find all eigenvalues of
\[a = \begin{bmatrix} -4 & 2 & -6 \\ 6 & 7 & 3 \\ 12 & -3 & 14 \end{bmatrix}\]
\begin{sol}
\[0 = \text{det}(A - \lambda I) = \begin{vmatrix}-4 - \lambda & 2 & - 6 \\ 6 & 7 - \lambda & 3 \\ 12 & -2 & 14 - \lambda \end{vmatrix} = \begin{vmatrix} -4 - \lambda & 2 & -6 \\ 6 & 7 - \lambda & 3 \\ 8 - \lambda & 0 & 8 - \lambda \end{vmatrix}\]
\[ = \begin{vmatrix} 2 - \lambda & 2 & -6 \\ 3 & 7 - \lambda & 3 \\ 0 & 0 & 8 - \lambda \end{vmatrix} =(8-\lambda)(-1)^{3+3} (\lambda^2 - 9 \lambda + 8) = -(\lambda - 8)(\lambda - 8)(\lambda - 1)\]
Therefore the eigenvalues are $\lambda_1 = 8, \lambda_2 = 8, \lambda_3 = 1$
\end{sol}
\end{exmp}



\end{document}