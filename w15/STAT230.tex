\documentclass[english, 12pt]{article}
\usepackage{yingconfig}

% ========================Variables======================================
\newcommand{\coursecode}{STAT 230}
\newcommand{\coursename}{Probability}
\newcommand{\thisprof}{Professor N. Mohammad}
\newcommand{\curterm}{Winter 2014}

\begin{document}
\notesheader
\section{Introduction}

\begin{defn}
\textbf{Probability} is the study of randomnesses and uncertainty.
\begin{itemize}
\item Variability in population, processes or phenomena
\end{itemize}
\end{defn}

\begin{defn}
\textbf{Classic Interpretatation} makes assumptions about the physical world to deduce probability.
\[ \f{\text{\# ways an event can occur}}{\text{Total \# of outcomes}}\]
\end{defn}

\begin{defn}
\textbf{Relative-frequency interpretation} is when the probability of specific outcome is defined as the proportion of times it occurs over the long run. May only be used if the experiment may be repeated.
\end{defn}

\begin{defn}
\textbf{Frequency} is just the amount of times an event occurs while \textbf{relative frequency} is a fraction of the amount of times an event occurs over the total amount of possible outcomes.
\end{defn}

\begin{defn}
\textbf{Personal-probability interpretation} is the degree to which a given individual believes the event wil happen.
\end{defn}

\begin{qte}
\textbf{Coherent} means that personal probability of one event does not contradict personal probability of another.
\end{qte}

\begin{exmp}
If the probability of finding a parking space is 0.2, the probability of not finding one should be 0.8.
\end{exmp}

\subsection{Probability Models}

\begin{defn}
\textbf{Experiment} is any action, phenomenon, or process whose outcome is subject to uncertainty.
\end{defn}

\begin{defn}
\textbf{Trial} is a single repetition of an experiment.
\end{defn}

\begin{defn}
\textbf{Sample space}, denoted by $S$, is the set of possible distinct outcomes. In a single trial, only one outcome may occur. The sample space may be either \textbf{discrete} or \textbf{continuous}.
\end{defn}

\begin{defn}
An \textbf{event} is any subset of outcomes contained in the sample space $S$.
\begin{itemize}
\item \textbf{Simple} - one outcome
\item \textbf{Compound} - more than one outcome
\end{itemize}
\end{defn}

\subsection*{Probability notation}
$P(\text{event})$ is used to denote the probability of an event occurring. The \textbf{compliment} is the probability of an event not happening and is denoted as $P(A^c)$ or $P(\overline{A})$
\begin{defn}
The odds in \textbf{favour} of an event is the odds of an event occurring compared to its compliment.
\[\f{P(A)}{P(A^c)} = \f{P(A)}{1-P(A)}\]
The odds against is the reciprocal of odds in favour.
\end{defn}

\begin{thrm}
\[\sum_{i=0}^k P(A_{i}) = 1\]
The set $P(A_{i}), i = 1,2,\dots$ is the probability distribution on $S$.
\end{thrm}

\begin{thrm}
If $A$ and $B$ are two events wirh $A \subseteq B$, then $P(A) \leq P(B)$
\end{thrm}

\begin{defn}
Two events are \textbf{mutually exclusive} or \textbf{disjoint} if they cannot happen simultaneously.
\[A \cap B = \emptyset \]
\end{defn}
\tabularnewline

\subsection{Counting Techniques}
Counting Principle $\bullet$ Permutations $\bullet$ Combinations\\\\

\begin{defn}
\textbf{Addition rule}: When there are $m$ ways to perform $A$, and $n$ ways to perform $B$, there are $m+n$ ways to perform $A$ \textbf{OR} $B$.
\end{defn}

\begin{defn}
\textbf{Product rule}: Where there are $p$ ways to perform $A$, and $q$ ways to perform $B$, there are $m \times n$ ways to perform $A$ \textbf{AND} $B$.
\end{defn}

\begin{defn}
\textbf{Uniform Probability Model}:
\[P(A) = \f{\text{Outcomes in } A}{\text{Outcomes in } S} \]
\end{defn}

\begin{defn}
\textbf{Permutation} is an arrangement of elements in an ordered list.
\begin{note}
The amount of ways to arrange $n$ items (all of them have to be used) is $n!$
\end{note}
\[P_{k,n} = \f{n!}{(n-k)!}\]
\end{defn}

\begin{defn}
Any unordered sequence of $k$ objects taken from a set of $n$ distinct objects is called a \textbf{combination}.
\[C_{k,n} = {n \choose k} = \f{n!}{k! (n-k)!}\]
\end{defn}

\begin{exmp}
\textbf{Binomial Theorem}:
\[(1+x)^n = {n \choose 0} + {n \choose 1} x + {n \choose 2} x^2 + \dots + {n \choose n} x^n\]
\end{exmp}

\section{Chapter 4}

\begin{defn}
The \textbf{union} of two events $A$ and $B$, $A \cup B$ is the set containing all outcomes in either $A$ or $B$.
\end{defn}

\begin{defn}
The \textbf{intersection} of two events $A$ and $B$, $ A \cup B$ is the set containing all outcomes that are in both $A$ and $B$.
\end{defn}

\begin{defn}
The \textbf{empty set}, $\emptyset$ is the set containing no outcomes.
\end{defn}

\begin{defn}
The probability of \textbf{either} event happening is the sum of their individual probabilities:
\[ P(A \cup B) = P(A) + P(B)\]
only if $A$ and $B$ are mutually exclusive.
\end{defn}

\begin{note}
For non-mutually exclusive events, $P(A \cup B) = P(A) + P(B) - P(A \cap B)$.
\[P(A \cup B \cup C) = P(A) + P(B) + P(C) - \lbrack P (A \cap B) + P(A \cap C) + P (B \cup C)\rbrack + P (A \cap B \cap C)\]
\end{note}

\begin{defn}
Two events are \textbf{independent} if and only if
\[ P(A \cap B) = P(A) P(B)\]
\end{defn}

\begin{defn}
The conditional probability of $A$ given $B$ is defined by
\[ P(A | B) = \f{P(A \cap B)}{P(B)}\]
\end{defn}

\begin{note}
If two events $A$ and $B$ are independent, then
\[P(A) = P(A | B)\]
\end{note}

\begin{qte}
Show all steps to combinations and permutation. (formulas)
\end{qte}

\begin{defn}
A set of events is \textbf{exhaustive} when at least one of the events must occur.
\end{defn}

\subsubsection*{Disease Problems}

\begin{defn}
Terminology:
\begin{itemize}
\item \textbf{False positive} is when test indicates positive and is wrong (true status is negative). $(T\,|\,D^c)$
\item \textbf{False negative} is when a test indicates a negative status and is wrong (true status is positive). $(T^c\,|\,D)$
\item \textbf{Sensitivity/ True positive} is when a test indicates a positive status and is correct. $(T\,|\,D)$
\item textbf{Specificity/ True negative} is when a test indicates a negative status and is wrong (true status is negative) $(T^c\,|\,D^c)$
\end{itemize}
\end{defn}

\begin{exmp}
A cheap blood test for HIV has the following characteristics:
\begin{itemize}
\item False positive rate is $0.5\%$
\item False negative rate is $2\%$
\item Around $0.04\%$ of Canadian males are infected with HIV
\end{itemize}
Determine the probability that if a male tests positive for HIV, he actually has HIV.
\begin{sol}
Let $D$ represent has disease and $T$ represent test positive. We are looking for $P(D\,|\,T)$. This is equivalent to
\[P(D\,|\,T) = \f{P(DT)}{P(T)}\]
Given information:
\[P(D) = 0.0004, P(D^c) = 0.9996\]
\[P(T^c\,|\,D)= 0.02, P(T\,|\,D) = 0.98\]
\[P(T\,|\,D^c) = 0.005, P(T^c\,|\,D^c) = 0.995\]
Now for the calculations
\[ P(DT) = P(T\,|\,D) \cdot P(D) = 0.98 \cdot 0.0004 = 0.000392\]
\[P(T) = P(DT) + P(D^cT) = P(DT) + P(T\,|\,D^c) \cdot P(D^c) = 0.000392 + 0.005 \cdot 0.9996 = 0.00539\]
\[P(D\,|\,T) = \f{0.000392}{0.00539} = 0.0727\]
\end{sol}
\end{exmp}

\begin{thrm}
\textbf{Bayes's Theorem} states: Let $A_{1},A_{2},\dots,A_{k}$ be mutually exclusive with prior probabilities $P(A_{i})\,i=1,2,\dots,k$. Then for any other event $B$ where $P(B) > 0$, the positerior probability of $A_{j}$ given that $B$ has occurred is
\[P(A_{j} | B) = \f{P(A_{J} \cap B)}{P(B)} \]
\end{thrm}

\section{Distributions}

\begin{defn}
A \textbf{variable} is any characteristic whose value may change from one object to another in the population. They are denoted by lowercase letters.
\end{defn}
\subsection*{Types of Variables}

\begin{defn}
\textbf{Numeric variable} is a quantitative variable.
\begin{itemize}
\item \textbf{Continuous variable}: Can take values consisting of an entire interval ith infinite number of real values. (eg: time)
\item \textbf{Discrete variable}: Can only take finite number of real values (even if limits may approach infinity). (eg number of people)
\end{itemize}
\end{defn}

\begin{defn}
\textbf{Categorical variables} have values that describe a quality or characeristic of a data unit.
\begin{itemize}
\item \textbf{Ordinal variables} take on values that can be ordered/ranked. (grades, clothing size)
\item \textbf{Nominal variables} take on value that are not able to be sequentialized. (gender)
\end{itemize}
\end{defn}

\begin{defn}
\textbf{Univariate Data} only involve a single variable. Main purpose of these is for central tendency analysis.
\end{defn}

\begin{defn}
\textbf{Bivariate data} involves two variables, and deals with causes or relationships. Analysis of variables simultaneously for correlation.
\end{defn}

\begin{defn}
\textbf{Multivariate data} involve more than two variables for more in depth analysis.
\end{defn}

\begin{defn}
A \textbf{random variable} is a function whose domain is the sample space and whose range is the set of possible values of the variable. It maps each outcome in the sample space with an outcome based on what the r.v. represents.

\begin{exmp}Let $X$ denote number of heads in two coin flips. $P(X = 0) = \f{1}{4}$.
\[\text{Sample space: } \{TT,HT,TH,HH\}\]

\begin{displaymath}
   X = \left\{
     \begin{array}{lr}
       0 : &TT \\
       1 : &HT,TH\\
       2 : &HH\\
     \end{array}
   \right.
\end{displaymath}
\end{exmp}
\end{defn}

\begin{defn}
The \textbf{probability function} of a random variable, $X$ is a function
\[f(x) = P(X=x)\qquad \forall x \in \mathbb{S}\]
Properties include:
\[f(x) \geq 0 \qquad \forall x \in \mathbb{S}\]
\[\sum_{x \in \mathbb{S}} f(x) = 1\]
\end{defn}

\begin{defn}
A \textbf{probability distribution} of $X$ is a description of the probabilities associated with all the possible values of $X$. It shows how the total probably of $1$ is distributed among the various values of $X$.
\end{defn}

\begin{defn}
The \textbf{cumulative distribution function} is defined by
\[F(x) = P(X \leq x) = \sum_i^x f(i)\]
For a number $x$, $F(x)$ is the probability that the observed value will be at most $x$.
\end{defn}

\begin{mthd}
To calculate the cdf, first $P(x)$ must be determined for all values of $x$. Then add each $f(x)$ to all the previous ones to obtain $P(x)$. For example if the range was $\{0,1,2\}$
\[ F(1) = P(X \leq 1) = P(1) + P(0)\]
\begin{displaymath}
   F(x) = \left\{
     \begin{array}{lr}
       \# : & x < 0 \\
       \# : & 0 \leq x < 1\\
       \# : &1 \leq x < 2\\
     \end{array}
   \right.
\end{displaymath}
\end{mthd}

\begin{note}
For any number $a$ and $b$ with $a \leq b$,
\[P(a \leq X \leq b) = F(b) - F(a-)\]
$a-$ represents the largest $X$ value that is strictly less than $a$.
\end{note}

\begin{note}
In addition: $f(x) = F(x) - F(x-1)$
\end{note}

\subsection{Uniform Distribution}
\begin{defn}
\textbf{Uniform distribution} is where each outcome has equal probability. If the range was from $a$ to $b$,
\begin{displaymath}
   P(X = x) = \left\{
     \begin{array}{lr}
       \f{1}{b-a+1} = \f{1}{n} &: x \in \lbrack a,b \rbrack\\
       0 &: x \not\in \lbrack a,b \rbrack \\
     \end{array}
   \right.
\end{displaymath}
Examples of this distribution include rolling a fair die, flipping a fair coin.
\end{defn}

\subsection{Binomial Distribution}
\begin{defn}
\textbf{Binomial distributions} are based on the probability experiments for which the results of each trial can be either a success or faillure. Requirements include:
\begin{itemize}
\item Experiment is repeated for fixed number of trials
\item Only two possible outcomes for each trial
\item Trials are independent
\item Probability of success is fixed
\item If sampling is without replacement, only use binomial distribution if the sample size is at most $5\%$ of the population size.
\end{itemize}
\end{defn}

\subsubsection*{Formula}
\begin{itemize}
\item $p$ denotes the probability of success in a trial.
\item $q = 1 - p$ denotes the probability of failure in a trial.
\item $X$ is the range of successes in n tries (where $x = 0,\cdots,n$)
\end{itemize}
\[P(X = x) = b(x;n,p) = {n \choose x} p^x (1-p)^{n-x},\,0 \leq x \leq n\]
We write $x\sim$ Binomial$(n,p)$.



\begin{defn}
The \textbf{Hypergeometric Distribution} is where a sample is taken from a finite population without replacement. Similar to binomial distribution, each trial has the possibility of success or failure. $N$ is the population, $n$ is the sample selected, $r$ is the total amount of successes available in the population, and $x$ represents the amount of successes within the selected sample.
\[P(X=x) = \f{{r\choose x} {n-1 \choose n - x}}{{N \choose n}},\,x = 0,1,\dots,n\]
\begin{qte}
Choose $x$ successes from the success samples, and out of the remaining failures, $N-r$, choose $n-x$ (failures in sample), all over the total amount of ways to select $n$ things in the population $N$.
\end{qte}
\end{defn}

\begin{defn}
The \textbf{negative binomial distribution} is based on an experiment satisfying thes conditions (similar to binomial)
\begin{itemize}
\item Independent trials
\item Only success or failure
\item P(success) is constant
\item \textbf{Experiment continues until k successes have occurred}
\end{itemize}
The rv in this distribution is the number of failures obtained before the kth success occurs.
\[ f(x) = P(X =x) = {x + k - 1 \choose x} p^k (1-p)^x\]
\end{defn}

\begin{defn}
The \textbf{geometric distribution} is a special case of negative binomial where $k$ is fixed at $1$.
\[f(x) = P(X=x) = p(1-p)^x\]
\end{defn}

\begin{defn}
The \textbf{Poisson Distribution} is used to model the number of events occurring within a given time. It can be used as a limiting case of the binomial distribution where $n$ is very large and $p$ is very small.
\[f(x) = \f{e^{-\lambda} \lambda^x}{x!},\, x = 0,1,2,\cdots,\infty\]
\begin{note}
$\lambda = np$ from a binomial distribution.
\end{note}
\begin{qte}
This approximation can be used as a substitute if $n > 50$ and $np < 5$.
\end{qte}
\end{defn}

\begin{defn}
\textbf{Poisson Process:} must satisfy three conditions.
\begin{itemize}
\item Independence
\item Individuality (chance of 2 or more events occuring in the same interval close to 0)
\item Homogencity: Events occur at uniform or homogeneous rate $\lambda$ over time
\end{itemize}
$\mu = \lambda t$
\end{defn}

\section{Value and Variance}

\begin{defn}
A \textbf{histogram} displays information on the frequency and relative frequency of each $x$ in the data set.
\end{defn}

\subsubsection*{Characteristics}
\begin{itemize}
\item \textbf{Symmetry}: if the graph is symmetrical.
\item \textbf{Unimodal}: if there is a single promiment peak. (Local maximum)
\item \textbf{Bimodal}: Two prominent peaks
\item \textbf{Multimodal}: More than two prominent peaks.
\end{itemize}

\begin{defn}
\textbf{Skewedness} refers to whether the data is pulled to wards a side. Could be positively skewed (towards left) or negatively skewed (towards right).
\end{defn}

\begin{defn}
The \textbf{arithmetic mean} denotes the arithmetic average value of observations. (Average)\\\
The \textbf{sample mean} measure the location, balance, and center of a sample.
\[\overline{x} = \f{1}{n} \sum_{i=1}^n x_{i}\]
The \textbf{population mean} is a weighted average in a probability distribution.
\[ \mu = \sum_{i=1}^N x_{i} \cdot f(x)\]
\end{defn}

\begin{defn}
The \textbf{median} is the middle value in the \textbf{ordered sample}. If there are two, take the average of the two.
\end{defn}

\begin{defn}
The \textbf{mode} is the value that occurs most often.
\end{defn}

\begin{defn}
\textbf{Outliers} are values that are very far from the rest of the data. Outlists affect the mean, but generally do not affect median and mode.
\end{defn}

\begin{defn}
\textbf{Variance} is a measure of the spread of the recorded values on a variable. It is a measure of dispersion. The \textbf{sample variance} is
\[ s^2 = \f{\sum_{i=1}^n (x_{i}- \overline{x})^2}{n-1}\]
For $N$ observations in a population, the \textbf{population variance} is
\[\sigma^2 = \sum_{i=1}^N (x_{i} - \mu)^2 \cdot f(x) = \f{\sum_{i=1}^N (x_{i} - \mu)^2}{N}\]
\end{defn}

\begin{defn}
$E(X)$ is the expected value of a distribution and is usually denoted as $\mu_{x}$.\\\\
Variance of $X$, denoted as $\sigma^2$ is
\[\sigma^2 = V(X) = \sum (x-\mu)^2 f(x) = E(X- \mu)^2 = \sum x^2 f(x) - \mu^2 = E(X^2) - \lbrack E(X) \rbrack ^2\]
\end{defn}
\[\text{Var} (X) = E(X^2) - \mu ^2\]
\[\text{Var}(x) = E(X(X-1)) + \mu - \mu^2\]

If $X$ is a discrete random variable with the probability function $f(x)$, then the expected value of a function $g(x)$ applied to all the values of $X$ is
\[E(g(x)) = \sum g(x) f(x)\]
\subsubsection*{Properties of Expected Value}
\[E(a\,g(X) + b) = a E(g(X)) + b\]
\subsubsection*{Rules of Variance}
\[V(aX + b) = a^2 \sigma^2_{x}\]
\[\sigma_{ax+b} = |a| \sigma_{x}\]

\begin{note}
The addition of a constant does not affect variance because it only changes location of mean value, but entire graph is shifted so spread is the same.
\end{note}

If $X$ \textasciitilde Bin$(n,p)$ then
\[\mu_{x} = E(X) = np\]
\[\sigma^2 = Var(X) = np(1-p)\]

If $X$ follows hypergeometric:
\[p = \f{r}{N}\]
\[E(X) = np\]
\[V(X) = \f{N-n}{N-1} np(1-p)\]

If $X$ follows Poisson distribution
\[\mu = \lambda\]
\[\sigma^2 = \lambda\]


If $X$ follows uniform distribution
\[\mu = \f{a+b}{2}\]
\[\sigma^2 = \f{(b-a+1)^2 - 1}{12}\]

If $X$ follows negative binomial
\[\mu = \f{kq}{p}\]
\[\sigma = \f{kq}{p^2}\]

\todo{cpt 7}

\section{Continuous Probability Distributions}

\begin{defn}
A random variable that can accept any value within an interval of numbers is \textbf{continuous}. Since there is an infinite amount of numbers between any two intervals, $P(X=x) = 0$ for each $x$ within the range.
\end{defn}

\begin{defn}
A \textbf{probability distribution} for a \textbf{continuous variable} $X$ is a function $f(x)$, and it measures an interval such that for any two numbers $a \leq b$,
\[P(a \leq X \leq b) = \int_a^b f(x)\,dx\]
\end{defn}

\begin{note}
Since $\int_a^a f(x) = 0$, $P(a \leq X \leq b) = P(a < X < b)$.
\end{note}

\begin{exmp}
Suppose that $X$ is a continuous rv with a probability density function
\begin{displaymath}
   f(x) = \left\{
     \begin{array}{lr}
       C(4x-2x^2) : 0 < x < 2 \\
       0 : \text{otherwise}
     \end{array}
   \right.
\end{displaymath}
\begin{itemize}
\item Find the value of $C$
\item Find $P(X > 1)$
\end{itemize}

\begin{sol}
\[C \int_0^2 (4x 02x^2)\,dx = 1\]
\[C (2x^2 - \f{2x^3}{3})\Big|_0^2 = 1\]
\[C = \f{3}{8}\]
\[f(x)=\f{3}{8}(4x-2x^2),\,0<x<2\]
Therefore
\[P (X > 1) = \int_1^2 f(x)\,dx = \f{3}{8} \int_1^2 (4x - 2x^2)\,dx = \f{1}{2}\]
\end{sol}
\end{exmp}

\begin{exmp}
Let the continuous rv $X$ denote the current measured in a thin wire in milliamperes. Assume that the range of $X$ is $\lbrack 0.200\rbrack$ mA, and the probability density function is
\begin{displaymath}
   f(x) = \left\{
     \begin{array}{lr}
       0.05 : 0 \leq x \leq 20 \\
       0 : \text{otherwise}
     \end{array}
   \right.
\end{displaymath}
What is the probability that a current measurement is less than $10$ milliamperes?

\begin{sol}
\[P (X < 10) = \int_0^{10} 0.05\,dx = 0.05 x\Big|_0^{10} = 0.5\]
\end{sol}
\end{exmp}

\begin{defn}
The \textbf{cumulative distribution function}, $F(x)$ for a \textbf{continuous random variable},$X$, is defined by
\[F(x) = P(X \leq x) = \int_{-\infty}^x f(y)\,dy\]
\end{defn}

\subsubsection*{Properties of cdf}
\begin{itemize}
\item $\lim_{x \to \pm \infty} F(x) = 0$
\item $F(x)$ is a non-decreasing function of $x$.
\item $P(a < X \leq b) = F(b) - F(a)$
\item $P(X = a) = 0$
\end{itemize}

\begin{exmp}
Utilizing the previous battery example, the cdf is
\[F(x) = \int_0^x f(u)\,du = 0.05x\]
\begin{displaymath}
   F(x) = \left\{
     \begin{array}{lr}
       0 : x < 0 \\
       0.05x : 0 \leq x < 20 \\
       1 : x \geq 20
     \end{array}
   \right.
\end{displaymath}
\end{exmp}

\begin{note}
If $X$ is a continuous rv with pdf $f(x)$, and cdf $F(x)$, then at every number for which $F'(x)$ exists,
\[F'(x) = f(x)\]
\end{note}

\begin{defn}
The \textbf{percentile} refers to the value for which a percentage of data falls below. For example being in the 99th percentile means that $99\%$ of people in the sample space are below you.
\end{defn}

\todo{Stopped at change of variable}

\begin{defn}
The \textbf{expected value} of a continuous random variable $X$ is,
\[E(X) = \int_{-\infty}^\infty x f(x)\,dx\]
Similarly, if $f(x)$ and $g(x)$ are functions of $X$, then
\[E(g(x)) = \int_{-\infty}^\infty g(x) f(x)\,dx\]
\end{defn}

\begin{exmp}
Find $E(X)$ if the probability density function is $f(x) = 2x,\, 0 \leq x \leq 1$.

\begin{sol}
\[E(X) = \int_{-\infty}^\infty x f(x)\,dx = \int_0^1 2x \cdot x\,dx = \f{2x^3}{3}\Big|_0^1 = \f{2}{3}\]
\end{sol}
\end{exmp}

\begin{exmp}
Suppose
\begin{displaymath}
   f(x) = \left\{
     \begin{array}{lr}
       1 : 0 \leq x \leq 1 \\
       0 : \text{otherwise}
     \end{array}
   \right.
\end{displaymath}

\begin{displaymath}
   h(x) = \left\{
     \begin{array}{lr}
       1-x : 0 \leq x < \f{1}{2} \\
       x : \f{1}{2} \leq x \leq 1
     \end{array}
   \right.
\end{displaymath}
Find E(h(x))

\begin{sol}
\[E(h(x)) = \int_0^{\f{1}{2}} (1-x)\cdot 1\,dx + \int_{-\f{1}{2}}^1 x \cdot 1\,dx\]
\[E(h(x)) = \left(x - \f{x^2}{2}\right)\Big|_0^{\f{1}{2}} + \left(\f{x^2}{2}\right)\Big|_{\f{1}{2}}^1\]
\end{sol}
\end{exmp}

\begin{defn}
If the random variable $X$ is continuous with the pdf $f(x)$, then the variance is
\[\text{Var}(X) = E((X-\mu)^2) = \int_{-\infty}^\infty (x-\mu)^2 f(x)\,dx = \int_{-\infty}^\infty x^2 f(x)\,dx - \mu^2\]
\end{defn}

\todo{Slide 42: variance of continuous function}

\todo{Up to normal distribution}

\begin{defn}
The normal distribution with parameter values $\mu = 0, \sigma = 1$ is the \textbf{standard normal distribution}. A random variable that follows this distribution is a standard normal random variable, and is denoted as $Z$.
\end{defn}

To convert a normal distribution to a standard score (z-score),
\[z = \f{\text{observed mean} - \text{mean}}{\text{standard distribution}}\]
\[z = \f{x - \mu}{\sigma}\]


\begin{note}
Since our z table does not have negatives, when calculating $\phi(-x)$, use $1 - \phi(x)$ since the distribution is symmetrical.
\end{note}

\todo{Start with cpt 8b}
\subsection{Exponential Distribution}
\begin{defn}
$X$ is said to follow \textbf{exponential distribution} if the pdf of $X$ is
\[f(x;\lambda) = \lambda e^{\lambda x}\]
The cdf for this function is $F(x;\lambda) = 1 - e^{- \lambda x}$. \\\\
It is used to describe the time or distance until some event happens.
\[E(X) = \mu = \f{1}{\lambda}\]
\[\text{Var}(X) = \f{1}{\lambda^2}\]

\begin{note}
If the \textbf{average rate of occurrence} in a Poisson process is given, $\lambda$ is used. If the \textbf{average waiting time for an occurrence} then $\theta$ is used. We use $\theta = \f{1}{\lambda}$
\end{note}
\end{defn}

\begin{exmp}
The time between arrivals of cars at a gas pump station follows an exponential distribution with an average time between arrivals of 3 minutes. What is the probability that the time between two successive arrivals will be 2 mins or less.

\begin{sol}
Must calculate $P(X \leq 2)$. From the question, $\theta = 3$, so $F(x) = 1 - e^{-\f{x}{3}}$. $F(2) = 1 - e^{-\f{2}{3}} = 0.4866$.
\end{sol}
\end{exmp}

\begin{note}
Exponential distributions are memoryless, so for example the probability of waiting 15 mins given that you have already waited 5 mins is the same as the probability without the initial waiting.
\[P((X > c + b)\,|\, X > b) = P(X > c)\]
\end{note}

\section{Multivariate Distributions}

\begin{defn}
Let $X$ and $Y$ represent two discrete random  variables in the sample space $\mathbb{S}$. The \textbf{joint probability function $f(x,y)$} is
\[f(x,y) = P(X = x \text{ and } Y = y) = P(X = x,Y = y)\]
\end{defn}

\begin{note}
If in a joint probability distribution, you are only asked to find one variable (eg: $P(X > 1)$), then you have to include all the possible values for the other variable (eg: $\sum^{\text{all y}} P(X > 1. Y = i)$).
\end{note}

\begin{defn}
The \textbf{marginal probability functions} of $X$ and $Y$, denoted $f_X(x)$ and $f_Y(y)$ are given by
\[f_X(a) = \sum^{\text{all y}} f(x,y)\]
\[f_Y(b) = \sum^{\text{all x}} f(x,y)\]
\end{defn}

\begin{exmp}
Let the joint distribution of $X$ and $Y$ be given by
\[f(x,y) = \f{x + y}{30}\qquad x = 0,1,2,3 \qquad y = 0,1,2\]
Find the marginal distribution function of $X$ and $Y$.

\begin{sol}
\[f_X(x) = \sum_{y = 0}^2 \f{x + y}{30} = \f{1}{30}((x + 0) + (x + 1) + (x + 2)) = \f{x+1}{10}\]

\begin{displaymath}
   f_X(x) = \left\{
     \begin{array}{lr}
       \f{x+1}{10} : & x = 0,1,2,3 \\
       0 : & \text{otherwise}\\
     \end{array}
   \right.
\end{displaymath}
Similarly, by computation,
\begin{displaymath}
   f_Y(y) = \left\{
     \begin{array}{lr}
       \f{2y+3}{15} : & y = 0,1,2 \\
       0 : & \text{otherwise}\\
     \end{array}
   \right.
\end{displaymath}

\begin{note}
An alternative solution would be to make a table and sum up the rows and columns, but that is a lot of work.
\end{note}
\end{sol}
\end{exmp}

\begin{defn}
The random variables $X,Y,\dots$ are \textbf{independent random variables} if for every subset of the variables, the joint probability function of the subset is equal to the product of the marginal probability functions.
\[f(x,y) = f(x) \cdot f(y)\qquad \forall x,y \in \mathbb{S}\]
\end{defn}

\begin{defn}
The conditional probability function of $X$ given $Y = y$ is given by
\[f(x|y) = \f{f(x,y)}{f_Y(y)}\]
and similarly
\[f(y|x) = \f{f(x,y)}{f_X(x)}\]
\end{defn}

\begin{exmp}
Suppose the joint probabililty function of $X$ and $Y$ is given by
\[f(0,0) = 0.4, f(0,1) = 0.2, f(1,0) = 0.1, f(1,1) = 0.3\]
Calculate the conditional probability function of $X$ given that $Y = 1$.

\begin{sol}
\[f_Y(1) = \sum^{\text{all x}} f(x,1) = f(0,1) + f(1,1) = 0.5\]
\[f_{X|Y}(0 | 1) = \f{f(0,1)}{f_Y(1)} = \f{0.2}{0.5} = \f{2}{5}\]
\[f_{X|Y}(1|1) = \f{f(1,1)}{f_Y(1)} = \f{0.3}{0.5} = \f{3}{5}\]
\end{sol}
\end{exmp}
\todo{p33, bi,poisson distribution with multivariable, and change of variable}

\subsection{Multinomial Distribution}

Suppose an experiment is repeated independently n times with k distinct types of outcome each time. Each experiment can result in any one of k possible outcomes, with probabilities $p_1,\dots,p_k$ each time. If $X$ denotes the number of times the ith type occurs, then $(X_1,\dots,X_k)$ has a multinomial distribution.
\[f(X_1 = x_1,\dots,X_k = x_k) = \f{n!}{x_1 !\cdots x_k !} p_{1}^{x1} \cdot p_{2}^{x2} \cdots p_{k}^{xk}\]

\begin{exmp}
Suppose that a fair die is rolled twelve times. The probability that 1 appears 4 times, 2 and 3 appear 3 times each, 4 and 5 appear once each, and 6 not at all is
\[\f{12!}{4!3!3!1!1!0!} (\f{1}{6})^4 (\f{1}{6})^3 (\f{1}{6})^3 (\f{1}{6})^1 (\f{1}{6})^1 (\f{1}{6})^0\]
\[= \f{12!}{4!3!3!} (\f{1}{6})^{12}\]
\end{exmp}

\todo{Some stuff in the middle}

\begin{defn}
If $X$ is a discrete random variable with probability function $f(x) = P(X = x)$, then the expected value is
\[E(X) = \mu_X = \sum^{\text{all x}} x f(x)\]
\[E(g(X)) = \sum^{\text{all x}} g(x) f(x)\]
Similarly,
\[E(XY) = \sum^{\text{all(x,y)}} xy f(x,y)\]
\[E(g,X,Y) = \sum^{\text{all(x,y)}} g(x,y) f(x,y)\]
\[E(X+Y) = E(X) + E(Y)\]
\end{defn}

If $X$ and $Y$ are independent, then for any functions $h$ and $g$,
\[E(g(X) h(Y)) = E(g(X)) E(h(Y))\]
\[E(ag(X,Y) + b h(X,Y)) = a E(g(X,Y)) + b E(h(X,Y))\]

\begin{defn}
\textbf{Deterministic relationship} has no variance nor uncertainty. If the value of one variable is known, the other variable can be determined exactly. (Eg: Unit conversions). Represented as $y = \beta_0 + \beta_{1} x$
\end{defn}

\begin{defn}
\textbf{Statistical relationship} has both variance and uncertainty. For a fixed value of $x$ there is unertainty in the value of the second variable $y$/
\end{defn}

\begin{defn}
\textbf{Regression analysis} is the part of statistics that investigates the relationship between two or more variables related in nondeterministic fashion. For this course, \textbf{covariance} and \textbf{correlation coefficient} are analyzed.
\end{defn}

\begin{defn}
The \textbf{covariance} between two random variables is
\[\text{Cov}(X,Y) = E \lbrack (X - \mu_X)(Y - \mu_Y) \rbrack\]
\[= \sum (x - \mu_X)(y-\mu_Y) f(x,y)\]
\[\text{Cov}(X,Y) = E(XY) - \mu_X \cdot \mu_Y\]
This measures how much two random variables change together.
\end{defn}

\begin{note}
If $X$ and $Y$ are independent, then $\text{Cov}(X,Y) = 0$, but the converse is not true.
\end{note}

\begin{defn}
The \textbf{correlation coefficient}, denoted as $\text{Corr}(X,Y)$, $\rho_{X,Y}$ or just $\rho$ is defined by
\[\rho_{X,Y} = \f{\text{Cov}(X,Y)}{\sigma_X \cdot \sigma_Y}\]
If $a$ and $c$ have the same sign (both positive/negative),
\[\text{Corr}(aX+b,cY+d) = \text{Corr}(X,Y)\]
The relationship is strong if $|\rho| > 0.8$, moderate if $0.5 < |\rho| < 0.8$, and weak if $|\rho| < 0.5$.
$\rho$ measures the degree of \textbf{linear relationship} between $X$ and $Y$.
\end{defn}


\end{document}

